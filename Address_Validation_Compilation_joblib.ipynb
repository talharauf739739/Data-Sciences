{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDMnzasdYdwx",
        "outputId": "934e163d-d9f5-4ac9-b3a6-eae0213d9d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Accuracy: 0.921\n",
            "Precision: 0.916548097763976\n",
            "ROC AUC: 0.8687372847678191\n",
            "R1-score: 0.921\n",
            "Best Parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
            "Training Completed and Model Saved as Address_Validator_Model.joblib\n"
          ]
        }
      ],
      "source": [
        "!pip install joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "import regex as re\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, roc_curve, auc, recall_score\n",
        "import joblib\n",
        "\n",
        "# Load the training data containing 'address', 'city', 'province', and 'Result' columns\n",
        "training_data = pd.read_excel('/content/Training_data .xlsx')\n",
        "\n",
        "# Concatenate 'address', 'city', and 'province' columns into a single 'full_address' column\n",
        "training_data['full_address'] = training_data['address'].astype(str) + ', ' + training_data['city'].astype(str) + ', ' + training_data['province'].astype(str)\n",
        "API_Match = training_data['API']\n",
        "\n",
        "# Define the replacements for the address column\n",
        "replacements = {\n",
        "    r'\\bH\\b': 'House',\n",
        "    r'\\bh\\b': 'house',\n",
        "    r'\\bH(?![oO])\\b': 'House',\n",
        "    r'\\bh(?![oO])\\b': 'house',\n",
        "    r'\\bst\\b': 'Street',\n",
        "    r'\\bSt\\b': 'Street',\n",
        "    r'\\bST\\b': 'Street',\n",
        "    r'\\bST(?![rR])\\b': 'Street',\n",
        "    r'\\bSt(?![rR])\\b': 'Street',\n",
        "    r'\\bst(?![rR])\\b': 'Street'\n",
        "}\n",
        "\n",
        "# Function to perform replacements\n",
        "def replace_words(text):\n",
        "    if isinstance(text, str):\n",
        "        for pattern, replacement in replacements.items():\n",
        "            text = re.sub(pattern, replacement, text)\n",
        "        return text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Apply replacements to each string in the 'full_address' column\n",
        "training_data['updated_address'] = training_data['full_address'].apply(replace_words)\n",
        "\n",
        "# Define feature extraction functions\n",
        "def extract_feature_1(text):\n",
        "    return 'Sector' in text or 'sector' in text or 'SECTOR' in text or 'Block' in text or 'BLOCK' in text or 'block' in text or 'House' in text or 'HOUSE' in text or 'house' in text or 'Street' in text or 'STREET' in text or 'street' in text or 'FLAT' in text or 'flat' in text or 'Flat' in text\n",
        "#'Sector|sector|SECTOR|Block|block|BLOCK|House|house|HOUSE|Street|street|STREET|Flat|flat|FLAT'\n",
        "\n",
        "def extract_feature_2(text):\n",
        "\n",
        "    return 'True' in API_Match\n",
        "\n",
        "def extract_feature_3(text):\n",
        "    return re.search(r'(House|house|HOUSE|H|no|NO|No|number|Number|n|N|Street|St|st|street|STREET) \\d', text) is not None\n",
        "\n",
        "# Extract features and add them as columns\n",
        "training_data['Featured_1'] = training_data['updated_address'].apply(extract_feature_1)\n",
        "training_data['Featured_2'] = training_data['updated_address'].apply(extract_feature_2)\n",
        "training_data['Featured_3'] = training_data['updated_address'].apply(extract_feature_3)\n",
        "\n",
        "# Use label encoding to convert textual labels to numerical values for the 'Result' column\n",
        "label_encoder = LabelEncoder()\n",
        "training_data['Result'] = label_encoder.fit_transform(training_data['Result'])\n",
        "\n",
        "# Drop unwanted columns\n",
        "training_data.drop(['order_id', 'province', 'city', 'address', 'status', 'updated_address', 'full_address'], axis=1, inplace=True)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = training_data.drop('Result', axis=1)\n",
        "y = training_data['Result']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    learning_rate=0.01,\n",
        "    n_estimators=500,\n",
        "    max_depth=300,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define the hyperparameters to search\n",
        "param_grid = {\n",
        "    'max_depth': [3, 6],\n",
        "    'n_estimators': [100, 300],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'subsample': [0.7, 0.8],\n",
        "    'colsample_bytree': [0.7, 0.8]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
        "\n",
        "# Perform the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_estimator.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the model\n",
        "y_pred_encoded = best_estimator.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_encoded)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, y_pred_encoded, average='weighted')\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "y_pred_prob = best_estimator.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Calculate R1-score\n",
        "recall = recall_score(y_test, y_pred_encoded, average='weighted')\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"ROC AUC:\", roc_auc)\n",
        "print(\"R1-score:\", recall)\n",
        "\n",
        "# Save the trained model to a file\n",
        "model_filename =  'Address_Validation_Model_Updated.joblib'  # Use .joblib extension\n",
        "joblib.dump(best_estimator, model_filename)\n",
        "with open(model_filename, 'wb') as model_file:\n",
        "    joblib.dump(best_estimator, model_file)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Training Completed and Model Saved as Address_Validator_Model.joblib\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2AdDm1P6Zu1e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}