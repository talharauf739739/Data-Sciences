{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b71a50d8-b311-4abb-9be6-c40292d5bf0a",
   "metadata": {},
   "source": [
    "> Including motorstate unique Brands 412, 461, 404, 425, 342, 132, 414, 337, 233, 481, 850, 867, 479, 690, 579 in price files\n",
    "\n",
    "> Activating Wheel Pros (putting 0 as Ship Cost)\n",
    "\n",
    "> Wheel Pros on BS Amazon - WP Shipping file correction\n",
    "\n",
    "> Bundle SKU Remove Duplicates enhancement\n",
    "\n",
    "> Turn 14\n",
    "\n",
    "> Black Burn inclusion in price files (shipping set as 0)\n",
    "\n",
    "> WTD Arroyo Shipping Estimates\n",
    "\n",
    "> DGTire Shipping Estimates\n",
    "\n",
    "> Inclusion of My Motor Madness\n",
    "\n",
    "> Inclusion of Dorman Manual Stock update\n",
    "\n",
    "> Inclusion of Tonsa Fedex based shipping Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rocky-october",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import exists\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(action='once')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-front",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup Stuff (that should come from config files or database later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698d8d9-6b58-4953-b6a4-dccf9b9da65f",
   "metadata": {},
   "source": [
    "CHANNEL: Setting up Channel Transaction Fee (%) for each Channel >> List of Dictionaries\n",
    "\n",
    "WAREHOUSES: Defining Keys and Shipping Charge Category for Each WareHouse >> List of Dictionaries\n",
    "\n",
    "get_warehouse_key(warehouse_name): Return Key of the given Warehouse (from the WAREHOUSES) - can be improved\n",
    "\n",
    "Initializing DEFAULT_TARGET_PROFIT(5%), DEFAULT_SHIP_MARKUP (12%), EXCLUDED_WAREHOUSES, PUNCTUATION_WAREHOUSES (Punctuation has to be retained in these)\n",
    "\n",
    "PARTS_AUTH_SHIPPING_MODEL: The Machine pretrained Machine Learning Model loaded from pickle\n",
    "PRICE_FILE_COLUMNS: Columns in the Price File\n",
    "\n",
    "PRICE_FILE_LOCATION\n",
    "CatSKU_CHANNELS // Need elaboration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jewish-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "CS_TRANSACTION_FEE = 0.0101\n",
    "\n",
    "CHANNELS = [\n",
    "    {'name':'AP Fusion', 'id':'APF'}, #Changed on 7th Oct\n",
    "    {'name':'PS Amazon', 'id':'PSA'},\n",
    "    {'name':'PS Walmart', 'id':'PSW'},\n",
    "    {'name':'PS Ebay', 'id':'PSE'},  ## Changed back to 0.12 here. The 0.02 for WHI will be added added separately for CatSKUs\n",
    "    {'name':'BS Amazon', 'id':'BSA'},\n",
    "    {'name':'BS Walmart', 'id':'BSW'},\n",
    "    {'name':'BS Ebay', 'id':'BSE'},\n",
    "    {'name':'Mecka', 'id':'BAABS'}, \n",
    "    {'name':'My Motor Madness', 'id':'MMM'} \n",
    "]\n",
    "\n",
    "WAREHOUSES = [\n",
    "    # Fully integrated warehouses\n",
    "    {'key':'C', 'name':'Brock', 'shipping':'free-ish'},\n",
    "    {'key':'D', 'name':'Dorman Direct', 'shipping':'free-ish'},\n",
    "    {'key':'J', 'name':'PFG', 'shipping':'theirs'},\n",
    "    {'key':'K', 'name':'Keystone', 'shipping':'theirs'},\n",
    "    {'key':'N', 'name':'NPW', 'shipping':'ours'},\n",
    "    {'key':'O', 'name':'Tonsa', 'shipping':'ours'},\n",
    "    {'key':'P', 'name':'Parts Auth', 'shipping':'theirs'},\n",
    "    {'key':'Y', 'name':'Motor State', 'shipping':'theirs'},\n",
    "    # Manual/FTP warehouses\n",
    "    {'key':'1', 'name':'Jante Wheel', 'shipping':'free'},\n",
    "    {'key':'2', 'name':'OE Wheels', 'shipping':'theirs'},\n",
    "    {'key':'6', 'name':'Burco Mirrors', 'shipping':'ours'},\n",
    "    {'key':'8', 'name':'Race Sport Lighting', 'shipping':'ours', 'target_profit': 0.1},\n",
    "    {'key':'9', 'name':'Sunbelt APG', 'shipping':'ours'},\n",
    "    {'key':'Z', 'name':'SimpleTire', 'shipping':'free'}\n",
    "    # Low-volume, or unused warehouses\n",
    "    #{'key':'5', 'name':'KSI Trading'},\n",
    "    #{'key':'7', 'name':'NTW'},\n",
    "    #{'key':'H', 'name':'Hanson'},\n",
    "    #{'key':'3', 'name':'Motor State'},\n",
    "]\n",
    "\n",
    "def get_warehouse_key(warehouse_name):\n",
    "    for warehouse in WAREHOUSES:\n",
    "        if warehouse['name'] == warehouse_name:\n",
    "            return warehouse['key']\n",
    "    return None\n",
    "\n",
    "\n",
    "DEFAULT_TARGET_PROFIT = 0.05\n",
    "DEFAULT_SHIP_MARKUP = 1 / 1.12\n",
    "EXCLUDED_WAREHOUSES = ['A','H', 'C']\n",
    "#EXCLUDED_WAREHOUSES = ['A','5','H','3', '7', '4', 'C', '3']\n",
    "PUNCTUATION_WAREHOUSES = ['J','1','C', '9', '8', 'Y', 'F', '7', 'U', '6', '4']\n",
    "\n",
    "PARTS_AUTH_SHIPPING_MODEL = 'shipping-research/tree-model.pkl'\n",
    "\n",
    "PRICE_FILE_COLUMNS = ['CS-SKU-NP', 'MinPrice', 'Shipping', 'Carrier', 'Service', 'Markup',\n",
    "       'ShipMkup', 'ListMkup', 'PackQty', 'MinQty', 'MaxQty', 'Zip Code',\n",
    "       'CatSKU', 'OP-Lowest(Y)', 'VND-Lowest(Y)', 'MinMkDown', 'MaxMkUp', 'Interval',\n",
    "       'BundleSKU', 'Duplicate']\n",
    "\n",
    "PRICE_FILE_LOCATION = 'price-files'\n",
    "\n",
    "#CatSKU_CHANNELS = ['PS Ebay']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-archive",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-retro",
   "metadata": {},
   "source": [
    "#### Load in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e732700-6cb3-4b1c-9e4b-06541d7e0297",
   "metadata": {},
   "source": [
    "Reading the Price Weight file from Disk\n",
    "\n",
    "warehouses: DataFrame containing unique warehouse keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "committed-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = pd.read_csv('inventory/pw-all.csv', low_memory=False, \n",
    "                 dtype={'MasterLC':'Int64', 'Zip Code': str})\n",
    "# pw['MasterLC'] = pw['MasterLC'].astype('Int64')\n",
    "\n",
    "# Temporarily remove all NPW.\n",
    "##pw = pw[pw['WD'] != 'N']\n",
    "\n",
    "warehouses = pw['WD'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-sessions",
   "metadata": {},
   "source": [
    "## Top-level processing and filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-diagram",
   "metadata": {},
   "source": [
    "#### Sad updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff36170-1614-4f0b-9189-ec9ba39ce5f8",
   "metadata": {},
   "source": [
    "In the Priceweight data (pw) copying CS-SKU to CS-SKU-NP - basically copying the SKU with punctuation into the non-punctuation SKU column (for Brock 'C')\\\n",
    "Setting MasterLC to 429 if MasterLC is 158 and 429\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c6d305-4405-4193-b7bc-487dfacbfa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the Line Code and CS-SKU-NP for Brock\n",
    "\n",
    "pw.loc[(pw['MasterLC']==158) & (pw['LC']=='429'), 'MasterLC'] = 429\n",
    "pw.loc[(pw['WD']=='C'), 'MasterSKU'] = pw.loc[(pw['WD']=='C'), 'MasterSKU'].str.replace('158|','429|', regex=False)\n",
    "pw.loc[(pw['WD']=='C'), 'CS-SKU'] = pw.loc[(pw['WD']=='C'), 'CS-SKU'].str.replace('158|','429|', regex=False)\n",
    "# pw.loc[pw['WD']=='C', 'CS-SKU-NP'] = pw.loc[pw['WD']=='C', 'CS-SKU']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-broadcasting",
   "metadata": {},
   "source": [
    "#### Preprocess price file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b5ae8-8128-4e86-9a01-6addedeccf89",
   "metadata": {},
   "source": [
    "Reading MAP (Minimum Advertized Price) from file against each SKU and dropping duplicates if any\\\n",
    "Sorting on 'CS-SKU-NP', dropping duplicates based on CS-SKU-NP and setting CS-SKU-NP as index -- This creates a unique sorted index which has performance benefits https://stackoverflow.com/questions/16626058/what-is-the-performance-impact-of-non-unique-indexes-in-pandas \\\n",
    "Basically, it can search any value in O(1) time\\\n",
    "In the end, we get a Pandas Series with SKU as index and MAP as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "endangered-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prices = pd.read_csv('maps.csv')\n",
    "map_prices = map_prices.sort_values('CS-SKU-NP').drop_duplicates(subset=['CS-SKU-NP'])\n",
    "map_prices = map_prices.set_index('CS-SKU-NP')['MAP']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c9737-5a4c-4253-a6af-8069cfab1fc6",
   "metadata": {},
   "source": [
    "Creating a new column 'item_cost' by copying 'MinPrice'\\\n",
    "Creating filter 'dorman_update_idx'=> where Warehouse is Dorman ('D') and PackQty is not na\n",
    "Using this filter to convert prices from per pack to per unit\n",
    "\n",
    "Creating backup copy of cs-sku-np with name cs-sku-np-catsku\n",
    "\n",
    "For Punctuation warehouses, Create CS-SKU-NP by concatenating Key, MasterLC, | and Part Number\\\n",
    "for non-Puncuation warehouses, remove non-alphanumeric characters from Part Number - This temporary step of copying to temporary variable 'x' can be avoided by filtering and assigning using loc function directly\n",
    "lambda operator defines a function in a single line - this function is passed as a filter to map\n",
    "\n",
    "In the Price Weight dataframe (pw), adding a new column 'MAP' (Minimum Advertized Price) with MAP prices from above step if available and 1 if not available in above data (map_prices)\\\n",
    "\n",
    "Removing WeatherTech (310) from Price Weight File\n",
    "\n",
    "Set not available values indicater (9999) back to actual nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sticky-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define separate column for managing costs vs MinPrice to avoid confusion.\n",
    "pw['item_cost'] = pw['MinPrice']\n",
    "\n",
    "# update Dorman costs (which are per-pack initally) to be per-unit\n",
    "# dorman_update_idx = (pw['WD']=='D') & (pw['PackQty'].notna())\n",
    "# pw.loc[dorman_update_idx, 'item_cost'] = pw.loc[dorman_update_idx, 'item_cost'] / pw.loc[dorman_update_idx, 'PackQty']\n",
    "# del dorman_update_idx\n",
    "pw.loc[pw['WD']=='D', 'PackQty'] = 1   #setting the PackQty for items from Dorman warehouse to 1 because they give us the price of the whole package - no need for Pack quantity multiplication\n",
    "\n",
    "pw['CS-SKU-NP'] = pw['WD-SKU']\n",
    "#Set CSSKUNP depending on if it's a punctuation warehouse\n",
    "x = pw[pw['WD'].isin(PUNCTUATION_WAREHOUSES)].copy()\n",
    "x['CS-SKU-NP'] = x['WD'] + x['MasterLC'].astype(str) + '|' + x['Part Number']\n",
    "pw.loc[pw['WD'].isin(PUNCTUATION_WAREHOUSES), :] = x\n",
    "\n",
    "pw = pw.loc[~pw['Part Number'].isnull()] #Removing null Part Numbers\n",
    "\n",
    "x = pw[~pw['WD'].isin(PUNCTUATION_WAREHOUSES)].copy()\n",
    "x['CS-SKU-NP'] = (x['WD'] + x['MasterLC'].astype(str) + '|' \n",
    "                  + x['Part Number'].map(lambda s: ''.join(filter(str.isalnum, s))))\n",
    "pw.loc[~pw['WD'].isin(PUNCTUATION_WAREHOUSES), :] = x\n",
    "\n",
    "pw = pw.join(other=map_prices, on='CS-SKU-NP', how='left')   #optimized code for fetching MAP columnt from map_prices\n",
    "pw['MAP'].fillna(1, inplace=True)\n",
    "del map_prices   #This dataset is no longer needed\n",
    "\n",
    "# Remove WeatherTech (just in case)\n",
    "pw = pw[pw['MasterLC'] != 310]\n",
    "\n",
    "# Remove First Stop Brakes Dorman Line\n",
    "# df = df[~((df['WD']=='D') & df['Part Number'].isin(first_stop_brakes))]\n",
    "# Nope, actually don't remove them, just set MinQty really high... at the end.\n",
    "\n",
    "# Remove placeholder values for Weight/ShipWeight\n",
    "pw.loc[(pw['Weight']==9999), 'Weight'] = np.nan\n",
    "pw.loc[(pw['ShipWeight']==9999), 'ShipWeight'] = np.nan\n",
    "\n",
    "pw['Shipping_Add'] = 0.0 #Creating a column for additional Shipping\n",
    "pw_cols = pw.columns   #columns in the Price Weight Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e4a4565-b002-4439-816d-81c8dc6f7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dorman Items pack Quantity Adjustment\n",
    "\n",
    "pw.loc[pw['MasterSKU']=='591|611034', 'PackQty'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-nirvana",
   "metadata": {},
   "source": [
    "#### Filter parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364dd923-0cd5-410e-9bc5-816c6cb80196",
   "metadata": {},
   "source": [
    "Remove Excluded warehouses from price weight data\\\n",
    "Adjusting invalid PackQty - na => 1\\\n",
    "Considering only values with PackQty <=10 or any Dorman values (we have clean data for Dorman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "progressive-prize",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Idea here is to filter out all the lil nasties that we don't want to include.\n",
    "# This could differ by warehouse, or not.\n",
    "# Things like, heavy parts, big or oddly shaped parts, \n",
    "# parts that are really expensive, or come in packs of many.\n",
    "# ... See notes on original Jim conversation for what all you should be including here.\n",
    "\n",
    "# Filter out excluded warehouses.\n",
    "pw = pw[~pw['WD'].isin(EXCLUDED_WAREHOUSES)]\n",
    "# Filter out nasty pack quantities. (allow these for Dorman, since we have clean data.)\n",
    "pw['PackQty'] = pw['PackQty'].fillna(1) # assume PackQty of NA => PackQty=1\n",
    "pw = pw[(pw['PackQty'] <= 10) | (pw['WD']=='D')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bcbf52-d5aa-48bc-85b6-a3ce7025344b",
   "metadata": {},
   "source": [
    "# Manual Inventories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a840a-9d8c-4851-be5c-538c53ce48f7",
   "metadata": {},
   "source": [
    "#### Brock Manual Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bfe870a-45f8-4c68-b46a-bf53a8c693ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pw1 = pw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f46ad7-de57-4b4d-94b0-1f85e8e3a5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbrock = pd.read_csv('inventories/brock_20220826.csv', low_memory=False)\\nbrock['CS-SKU'] = 'C' + brock['sku_filtered_cs']\\nbrock = brock[['CS-SKU', 'prices', 'total_stock']]\\n\\n#Merging with the Brock inventory\\npw = pw.merge(brock, how='left', left_on='CS-SKU-NP', right_on='CS-SKU', suffixes=('','_b') )\\n\\npw.loc[~pw['prices'].isnull(), 'MinPrice'] = pw.loc[~pw['prices'].isnull(), 'prices']\\npw.loc[~pw['prices'].isnull(), 'item_cost'] = pw.loc[~pw['prices'].isnull(), 'prices']\\n\\n#updating stock from the brock inventory\\npw.loc[~pw['prices'].isnull(), 'Qty'] = pw.loc[~pw['prices'].isnull(), 'total_stock']\\n\\npw.loc[((pw['prices'].isnull()) & (pw['WD']=='C')), 'Qty'] = 0  #Setting Quantity of items not present in inventory file to zero\\n\\n\\n#Restoring pw columns from before brock manual update\\npw = pw[pw_cols]\\n\\ndel brock #Delete Brock Inventory Dataframe as it is no longer needed\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "brock = pd.read_csv('inventories/brock_20220826.csv', low_memory=False)\n",
    "brock['CS-SKU'] = 'C' + brock['sku_filtered_cs']\n",
    "brock = brock[['CS-SKU', 'prices', 'total_stock']]\n",
    "\n",
    "#Merging with the Brock inventory\n",
    "pw = pw.merge(brock, how='left', left_on='CS-SKU-NP', right_on='CS-SKU', suffixes=('','_b') )\n",
    "\n",
    "pw.loc[~pw['prices'].isnull(), 'MinPrice'] = pw.loc[~pw['prices'].isnull(), 'prices']\n",
    "pw.loc[~pw['prices'].isnull(), 'item_cost'] = pw.loc[~pw['prices'].isnull(), 'prices']\n",
    "\n",
    "#updating stock from the brock inventory\n",
    "pw.loc[~pw['prices'].isnull(), 'Qty'] = pw.loc[~pw['prices'].isnull(), 'total_stock']\n",
    "\n",
    "pw.loc[((pw['prices'].isnull()) & (pw['WD']=='C')), 'Qty'] = 0  #Setting Quantity of items not present in inventory file to zero\n",
    "\n",
    "\n",
    "#Restoring pw columns from before brock manual update\n",
    "pw = pw[pw_cols]\n",
    "\n",
    "del brock #Delete Brock Inventory Dataframe as it is no longer needed\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97162be6-8f8c-4bae-92f3-749083f8826d",
   "metadata": {},
   "source": [
    "#### NPW Manual Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060f45e0-e0cc-4844-a8fa-1c2ef9b1fdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnpw = pd.read_csv('inventories/npw_20220826.csv', low_memory=False)\\nnpw['Core'].fillna(0, inplace=True)\\nnpw['WD'] = 'N'\\nnpw.rename(columns={'PartN':'Part Number', 'LineCode':'LC'}, inplace=True )\\nnpw['Cost'] += npw['Core']   #Add core amount to Cost\\nnpw.drop(columns = ['Core'], inplace=True) #Drop the Core column as it has already been added to the cost\\nnpw['Part Number'] = npw['Part Number'].map(lambda s: ''.join(filter(str.isalnum, s)))   #Removing Punctuation from NPW Part Numbers\\n\\npw = pw.merge(npw, how='left', on = ['WD', 'LC', 'Part Number'], suffixes=('', '_npw') )   #Merging with NPW inventory\\npw.loc[~pw['QA'].isnull(), 'Qty'] = pw.loc[~pw['QA'].isnull(), 'QA']   #Updating Quantity directly from NPW inventory\\npw.loc[~pw['QA'].isnull(), 'MinPrice'] = pw.loc[~pw['QA'].isnull(), 'Cost']   #Updating MinPrice and Cost directly from NPW Inventory\\npw.loc[~pw['QA'].isnull(), 'item_cost'] = pw.loc[~pw['QA'].isnull(), 'Cost']\\n\\npw.loc[((pw['QA'].isnull()) & (pw['WD']=='N')), 'Qty'] = 0   #Setting Quantity for NPW items not found in NPW inventory to 0\\n\\npw = pw[pw_cols]   #Restoring pw columns from before npw manual update\\ndel npw   #Delete npw Inventory Dataframe as it is no longer needed\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "npw = pd.read_csv('inventories/npw_20220826.csv', low_memory=False)\n",
    "npw['Core'].fillna(0, inplace=True)\n",
    "npw['WD'] = 'N'\n",
    "npw.rename(columns={'PartN':'Part Number', 'LineCode':'LC'}, inplace=True )\n",
    "npw['Cost'] += npw['Core']   #Add core amount to Cost\n",
    "npw.drop(columns = ['Core'], inplace=True) #Drop the Core column as it has already been added to the cost\n",
    "npw['Part Number'] = npw['Part Number'].map(lambda s: ''.join(filter(str.isalnum, s)))   #Removing Punctuation from NPW Part Numbers\n",
    "\n",
    "pw = pw.merge(npw, how='left', on = ['WD', 'LC', 'Part Number'], suffixes=('', '_npw') )   #Merging with NPW inventory\n",
    "pw.loc[~pw['QA'].isnull(), 'Qty'] = pw.loc[~pw['QA'].isnull(), 'QA']   #Updating Quantity directly from NPW inventory\n",
    "pw.loc[~pw['QA'].isnull(), 'MinPrice'] = pw.loc[~pw['QA'].isnull(), 'Cost']   #Updating MinPrice and Cost directly from NPW Inventory\n",
    "pw.loc[~pw['QA'].isnull(), 'item_cost'] = pw.loc[~pw['QA'].isnull(), 'Cost']\n",
    "\n",
    "pw.loc[((pw['QA'].isnull()) & (pw['WD']=='N')), 'Qty'] = 0   #Setting Quantity for NPW items not found in NPW inventory to 0\n",
    "\n",
    "pw = pw[pw_cols]   #Restoring pw columns from before npw manual update\n",
    "del npw   #Delete npw Inventory Dataframe as it is no longer needed\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699cc19e-bf33-4386-8158-689bac39606e",
   "metadata": {},
   "source": [
    "#### PFG Manual Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6670683-d6da-4a16-a8e6-ee78dab37434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\npfg = pd.read_csv('inventory/pfg.txt', sep='\\t', encoding_errors='ignore', escapechar='\\\\', low_memory=False, usecols=['SKU', 'STOCK_TOTAL', 'COST'] )\\npfg['WD'] = 'J'\\npfg.set_index( keys=['WD', 'SKU'], inplace=True)\\n\\npw = pw.join(other=pfg, on=['WD', 'Part Number'], how='left')   #Joining the price weight report with PFG inventory\\n\\npfg_inv_fil = (pw['WD']=='J') & (~(pw['STOCK_TOTAL'].isnull()))   #creating filter for items found in PFG inventory\\n\\n#Updating Stock and Cost from PFG inventory into the price weight report\\npw.loc[pfg_inv_fil, 'MinPrice'] =pw.loc[pfg_inv_fil, 'COST']\\npw.loc[pfg_inv_fil, 'item_cost'] =pw.loc[pfg_inv_fil, 'COST']\\npw.loc[pfg_inv_fil, 'Qty'] =pw.loc[pfg_inv_fil, 'STOCK_TOTAL']\\n\\npw.loc[(pw['WD']=='J') & (pw['STOCK_TOTAL'].isnull()), 'Qty'] = 0   #Setting the stock of PFG items not in PFG inventory to 0\\npw.drop(columns=['STOCK_TOTAL', 'COST'], inplace=True)\\n\\ndel pfg_inv_fil\\ndel pfg\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''    \n",
    "pfg = pd.read_csv('inventory/pfg.txt', sep='\\t', encoding_errors='ignore', escapechar='\\\\', low_memory=False, usecols=['SKU', 'STOCK_TOTAL', 'COST'] )\n",
    "pfg['WD'] = 'J'\n",
    "pfg.set_index( keys=['WD', 'SKU'], inplace=True)\n",
    "\n",
    "pw = pw.join(other=pfg, on=['WD', 'Part Number'], how='left')   #Joining the price weight report with PFG inventory\n",
    "\n",
    "pfg_inv_fil = (pw['WD']=='J') & (~(pw['STOCK_TOTAL'].isnull()))   #creating filter for items found in PFG inventory\n",
    "\n",
    "#Updating Stock and Cost from PFG inventory into the price weight report\n",
    "pw.loc[pfg_inv_fil, 'MinPrice'] =pw.loc[pfg_inv_fil, 'COST']\n",
    "pw.loc[pfg_inv_fil, 'item_cost'] =pw.loc[pfg_inv_fil, 'COST']\n",
    "pw.loc[pfg_inv_fil, 'Qty'] =pw.loc[pfg_inv_fil, 'STOCK_TOTAL']\n",
    "\n",
    "pw.loc[(pw['WD']=='J') & (pw['STOCK_TOTAL'].isnull()), 'Qty'] = 0   #Setting the stock of PFG items not in PFG inventory to 0\n",
    "pw.drop(columns=['STOCK_TOTAL', 'COST'], inplace=True)\n",
    "\n",
    "del pfg_inv_fil\n",
    "del pfg\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4d1eb-92bb-403b-bd70-b0424e06808d",
   "metadata": {},
   "source": [
    "#### Dorman Manual Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33147955-5b7d-48af-beac-aef8fc850f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndprice = pd.read_csv('inventory/dorman_price.csv', low_memory=False)\\ndprice['WD'] = 'D'\\ndprice['COST'] = dprice['Cost'] + dprice['Core']\\ndprice.drop(columns=['Cost', 'Core'], inplace=True)\\ndprice['PN'] = dprice['PN'].map(lambda s: ''.join(filter(str.isalnum, s)))\\ndprice.set_index(['WD', 'LC', 'PN'], inplace=True)\\n\\npw = pw.join(dprice, on=['WD', 'LC', 'Part Number'], how='left')   #Joining pw-all with Dorman Price File\\n\\npw.loc[~pw['COST'].isnull(), 'MinPrice'] = pw.loc[~pw['COST'].isnull(), 'COST']  #Updating Prices from Dorman Price File\\npw.loc[~pw['COST'].isnull(), 'item_cost'] = pw.loc[~pw['COST'].isnull(), 'COST']\\npw.drop(columns='COST', inplace=True)\\ndel dprice\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dprice = pd.read_csv('inventory/dorman_price.csv', low_memory=False)\n",
    "dprice['WD'] = 'D'\n",
    "dprice['COST'] = dprice['Cost'] + dprice['Core']\n",
    "dprice.drop(columns=['Cost', 'Core'], inplace=True)\n",
    "dprice['PN'] = dprice['PN'].map(lambda s: ''.join(filter(str.isalnum, s)))\n",
    "dprice.set_index(['WD', 'LC', 'PN'], inplace=True)\n",
    "\n",
    "pw = pw.join(dprice, on=['WD', 'LC', 'Part Number'], how='left')   #Joining pw-all with Dorman Price File\n",
    "\n",
    "pw.loc[~pw['COST'].isnull(), 'MinPrice'] = pw.loc[~pw['COST'].isnull(), 'COST']  #Updating Prices from Dorman Price File\n",
    "pw.loc[~pw['COST'].isnull(), 'item_cost'] = pw.loc[~pw['COST'].isnull(), 'COST']\n",
    "pw.drop(columns='COST', inplace=True)\n",
    "del dprice\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48c202-a36e-4821-b4fe-e79311bb0884",
   "metadata": {},
   "source": [
    "#### Dorman Manual Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09fd42a5-4c09-4ac7-b983-9f2b02d5ba5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndstock = pd.read_csv('inventory/dorman_stock.csv', low_memory=False)\\ndstock.columns.values[0] = 'PN'   #Renaming the Part Number column to PN\\ndstock.columns.values[1] = 'Stock'   #Renaming the Stock column\\ndstock['WD'] = 'D'\\ndstock['LC'] = 'DOR'\\ndstock = dstock[['WD', 'LC', 'PN', 'Stock']]  #Keepin gonly the required columns\\n\\ndstock['PN'] = dstock['PN'].map(lambda s: ''.join(filter(str.isalnum, s)))   #Removing punctuation from Part Number\\ndstock.drop_duplicates(subset=['PN'], inplace=True)   #Removing the duplicate parts\\ndstock.set_index(['WD', 'LC', 'PN'], inplace=True)\\n#Applying stock definitions\\ndstock.loc[dstock['Stock']=='In Stock', 'Qty'] = 5\\ndstock.loc[dstock['Stock']=='Low Stock', 'Qty'] = 2\\ndstock.loc[dstock['Stock']=='Out of Stock', 'Qty'] = 0\\ndstock['Qty'] = dstock['Qty'].astype('int')\\ndstock.drop(columns='Stock', inplace=True)\\n\\npw = pw.join(dstock, on=['WD', 'LC', 'Part Number'], how='left', rsuffix='_d')   #Joining pw-all with Dorman Price File\\npw.loc[~pw['Qty_d'].isna(), 'Qty'] = pw.loc[~pw['Qty_d'].isna(), 'Qty_d']   #Setting the Quantity from Dorman inventory to pw report\\npw.drop(columns='Qty_d', inplace=True)   #Removing the inventory column\\ndel dstock   #Removing the dorman inventory data frame as it is no longer needed\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dstock = pd.read_csv('inventory/dorman_stock.csv', low_memory=False)\n",
    "dstock.columns.values[0] = 'PN'   #Renaming the Part Number column to PN\n",
    "dstock.columns.values[1] = 'Stock'   #Renaming the Stock column\n",
    "dstock['WD'] = 'D'\n",
    "dstock['LC'] = 'DOR'\n",
    "dstock = dstock[['WD', 'LC', 'PN', 'Stock']]  #Keepin gonly the required columns\n",
    "\n",
    "dstock['PN'] = dstock['PN'].map(lambda s: ''.join(filter(str.isalnum, s)))   #Removing punctuation from Part Number\n",
    "dstock.drop_duplicates(subset=['PN'], inplace=True)   #Removing the duplicate parts\n",
    "dstock.set_index(['WD', 'LC', 'PN'], inplace=True)\n",
    "#Applying stock definitions\n",
    "dstock.loc[dstock['Stock']=='In Stock', 'Qty'] = 5\n",
    "dstock.loc[dstock['Stock']=='Low Stock', 'Qty'] = 2\n",
    "dstock.loc[dstock['Stock']=='Out of Stock', 'Qty'] = 0\n",
    "dstock['Qty'] = dstock['Qty'].astype('int')\n",
    "dstock.drop(columns='Stock', inplace=True)\n",
    "\n",
    "pw = pw.join(dstock, on=['WD', 'LC', 'Part Number'], how='left', rsuffix='_d')   #Joining pw-all with Dorman Price File\n",
    "pw.loc[~pw['Qty_d'].isna(), 'Qty'] = pw.loc[~pw['Qty_d'].isna(), 'Qty_d']   #Setting the Quantity from Dorman inventory to pw report\n",
    "pw.drop(columns='Qty_d', inplace=True)   #Removing the inventory column\n",
    "del dstock   #Removing the dorman inventory data frame as it is no longer needed\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc42d8-25eb-4ebb-b4d0-c835b963b873",
   "metadata": {},
   "source": [
    "Manual Update verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b1ae879-0d0d-42db-83f7-b9bb6df5eff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntmp = pw.merge (pw1, on=[\\'WD\\', \\'LC\\', \\'Part Number\\'], suffixes = (\\'_n\\', \\'_p\\' ) )\\nprint (tmp.query(\"MinPrice_n != MinPrice_p\").groupby(\\'WD\\').size())\\nprint (tmp.query(\"Qty_n != Qty_p\").groupby(\\'WD\\').size())\\ndel tmp\\n#del pw1\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tmp = pw.merge (pw1, on=['WD', 'LC', 'Part Number'], suffixes = ('_n', '_p' ) )\n",
    "print (tmp.query(\"MinPrice_n != MinPrice_p\").groupby('WD').size())\n",
    "print (tmp.query(\"Qty_n != Qty_p\").groupby('WD').size())\n",
    "del tmp\n",
    "#del pw1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-palestine",
   "metadata": {},
   "source": [
    "#### Calculate shipping by warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e7348-303d-4ca8-be38-a0fbb3b453ae",
   "metadata": {},
   "source": [
    "Read Shipping Data from file\\\n",
    "create column 'warehouse_key' in this dataframe by fetching key using the 'get_warehouse_key' function\n",
    "Filter to include entries with Quantity >0 - exclude Quantities=0\\\n",
    "Converting 'Ship Cost' to per item\n",
    "Filtering records with Shipping Cost > 0.05 and only considering a few columns\n",
    "\n",
    "Reading price-file-shipping.csv and converting it all to a single dictionary | Using a series with unique index should give us the same performance\n",
    "\n",
    "#### get_historical_shipping_estimates(df, warehouse_key):\n",
    "    Filter the main (total) shipping data to the provided warehouse, join/merge df to this filtered shipping data based on SKU creating 'ship_weights'. Filter the records with weights between 0 and 1000 (removing garbage).\n",
    "    If some clean data is available after this filtering, fit a LinearRegression() model on it to prodict Ship Cost based on dimensions and weights of the items. Displaying the RMS error of this trained model - Train, Test split missing here\n",
    "    \n",
    "##### Shipping cost prediction:\n",
    "    if the sku is available in the recent price-file-shipping, use that shipping cost,\n",
    "    if their is an order history of the sku, use the mean of the shipping cost of all historical orders\n",
    "    if the shipping cost (LinearRegression) model exists for the warehouse (i.e. clean data is avaialable), predict the shipping cost using this model\n",
    "    if their is some historical data available for that warehouse, use its mean\n",
    "    else use 10 as shipping cost\n",
    "    \n",
    "#### calculate_warehouse_shipping(df, warehouse):\n",
    "Expects a price/ weight dataframe format filtered for the warehouse. Returns with shipping price altered.\n",
    "Assigns shipping values for each warehouse as per its rules.\\\n",
    "##### pfg\n",
    "Checks the pfg inventory file to check for shipping and handling costs of the items. Sums these two costs and adds $3.5 to account for taxes. There should be some other way then this loop. (A simple join would probably be better for performance). If the item isn't found in the inventory file, use PFG_DEFAULT_SHIPPING (15)\n",
    "\n",
    "##### Keystone\n",
    "KEYSTONE_BASE_SHIPPING=11, KEYSTONE_LTL_SHIPPING=125\n",
    "\n",
    "##### NPW\n",
    "Possible overwrite of AC Delco SKUs (ignoring the 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb2e33cd-cf51-4084-98dc-d8a4f2573d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipping_data = pd.read_csv('shipping_data.csv', low_memory=False, parse_dates=['Ship Date'])\n",
    "shipping_data['warehouse_key'] = shipping_data['Warehouse'].map(get_warehouse_key)\n",
    "shipping_data = shipping_data[shipping_data['Quantity'] > 0]\n",
    "shipping_data['Ship Cost'] = shipping_data['Ship Cost'] / shipping_data['Quantity']\n",
    "shipping_data = shipping_data[shipping_data['Ship Cost'] > 0.05][['CS-SKU','warehouse_key','Ship Cost','Ship Date']]\n",
    "\n",
    "price_file_shipping = pd.read_csv('price-file-shipping.csv', index_col='CS-SKU-NP')   #Optimized code instead of loop for dictionary conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb769e85-1bae-4168-859b-33d21863854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_shipping_estimates(df, warehouse_key): # (cssku, warehouse_key):    \n",
    "    warehouse_shipping_data = shipping_data[(shipping_data['warehouse_key']==warehouse_key)].copy()\n",
    "\n",
    "    # create warehouse-level shipping model\n",
    "    feature_cols = ['Weight', 'DimWeight', 'ShipWeight', 'Length', 'Width', 'Height']\n",
    "    ship_weights = df[['MasterSKU']+feature_cols].merge(warehouse_shipping_data, how='inner',\n",
    "                                                        left_on='MasterSKU', right_on='CS-SKU').copy().dropna()\n",
    "    ship_weights = ship_weights[(ship_weights['ShipWeight'] > 0) & (ship_weights['ShipWeight'] < 1000)]  \n",
    "    if len(ship_weights) > 0:\n",
    "        model = LinearRegression().fit(ship_weights[feature_cols], ship_weights['Ship Cost'])\n",
    "\n",
    "         # log model error for audit purposes\n",
    "        print('RMSE:',mean_squared_error(ship_weights['Ship Cost'], \n",
    "                                          model.predict(ship_weights[feature_cols]), squared=False))\n",
    "    else:\n",
    "         model = None\n",
    "\n",
    "\n",
    "    #Getting Shipping price from recent price file\n",
    "    df['cssku'] = df['WD']+df['MasterSKU']   #Creating cssku to fetch data from recent price file\n",
    "    df = df.join(other=price_file_shipping, on='cssku', how='left', rsuffix='_pfs')\n",
    "    df['Shipping'] = df['Shipping_pfs']\n",
    "    df.drop(columns=['cssku', 'Shipping_pfs'], inplace=True)\n",
    "\n",
    "    #Getting Mean of Shipping from Historical Shipping Data\n",
    "    h_ship_avg = warehouse_shipping_data.groupby('CS-SKU').mean()['Ship Cost']\n",
    "    df = df.join(other=h_ship_avg, how='left', on='MasterSKU')\n",
    "    df.loc[df['Shipping'].isna(), 'Shipping'] = df.loc[df['Shipping'].isna(), 'Ship Cost']\n",
    "    df.drop(columns=['Ship Cost'], inplace=True)\n",
    "\n",
    "    #Getting Shipping Prediction from Dimensional LinearRegression model\n",
    "    if len(ship_weights) > 0:\n",
    "        df['ship_model'] = model.predict(df[feature_cols].fillna(0))\n",
    "        df.loc[df['Shipping'].isna(), 'Shipping'] = df.loc[df['Shipping'].isna(), 'ship_model']\n",
    "        df.drop(columns=['ship_model'], inplace=True)\n",
    "\n",
    "    \n",
    "    #Getting Warehouse Shipping Mean\n",
    "    h_warehouse_avg = warehouse_shipping_data.groupby('warehouse_key').mean()['Ship Cost']\n",
    "    df = df.join(other=h_warehouse_avg, on='WD')\n",
    "    df.loc[df['Shipping'].isna(), 'Shipping'] = df.loc[df['Shipping'].isna(), 'Ship Cost']\n",
    "    df.drop(columns=['Ship Cost'], inplace=True)\n",
    "\n",
    "    #Baseline value of 10\n",
    "    df.loc[df['Shipping'].isna(), 'Shipping'] = 10\n",
    "    \n",
    "    return df['Shipping'].values\n",
    "\n",
    "# Expects something formatted like a price/weight DF, filtered for a warehouse\n",
    "# returns the price/weight DF with shipping altered\n",
    "def calculate_warehouse_shipping(df, warehouse):\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(warehouse)\n",
    "    if warehouse=='D': # Dorman        \n",
    "        #df.loc[(df['item_cost'] <= 30), ['Shipping', 'ShipMkup']] = 6, DEFAULT_SHIP_MARKUP # flat rate\n",
    "        #df.loc[(df['item_cost'] > 30), ['Shipping', 'ShipMkup']] = 0, 1\n",
    "        df.loc[:, ['Shipping', 'ShipMkup']] = 0, 1\n",
    "        #df['ShipMkup'] = 1\n",
    "        \n",
    "    elif warehouse=='C': # Brock\n",
    "        df.loc[(df['item_cost'] <= 50), 'Shipping'] = 12 # estimate / avg.\n",
    "        df.loc[(df['item_cost'] > 50), 'Shipping'] = 0\n",
    "        df['ShipMkup'] = 1 / 1.1 # to account for returns not being accepted\n",
    "    elif warehouse=='P': # Parts Auth\n",
    "        df['Shipping'] = get_historical_shipping_estimates(df, warehouse)\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "        '''\n",
    "        with open(PARTS_AUTH_SHIPPING_MODEL, 'rb') as f:\n",
    "            m = pickle.load(f)\n",
    "        df['lwh'] = df['Length'] * df['Width'] * df['Height']\n",
    "        df['Shipping'] = m.predict(df[['Weight','Length','Width','Height','lwh']].fillna(0))\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "        '''\n",
    "    elif warehouse=='1': # Jante\n",
    "        df['Shipping'] = 0\n",
    "        df['ShipMkup'] = 1\n",
    "    elif warehouse=='J': # PFG        \n",
    "        #Reading Shipping and Handling Cost from PGF inventory file and using their sum as Shipping Cost\n",
    "        PFG_DEFAULT_SHIPPING = 15\n",
    "        inv = pd.read_csv('inventory/pfg.txt', sep='\\t', encoding_errors='ignore', escapechar='\\\\', low_memory=False, usecols=['SKU','SHIPPING_COST','HANDLING_COST'], index_col='SKU' )\n",
    "        inv['pfg_cost'] = inv[['SHIPPING_COST', 'HANDLING_COST']].sum(1)\n",
    "        inv.drop(columns=['SHIPPING_COST','HANDLING_COST'], inplace=True)\n",
    "        df = df.join(other=inv, on='Part Number', how='left')\n",
    "        df.loc[df['WD']=='J', 'pfg_cost'].fillna(PFG_DEFAULT_SHIPPING, inplace=True)\n",
    "        df.loc[df['WD']=='J', 'Shipping'] = df.loc[df['WD']=='J', 'pfg_cost']\n",
    "        df.drop(columns='pfg_cost', inplace=True)\n",
    "\n",
    "        # ADD AN EXTRA $3.50 TO ACCOUNT FOR TAX MESS\n",
    "        #df['Shipping'] = df['Shipping'] + 3.50 \n",
    "        \n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "    elif warehouse=='K': # Keystone\n",
    "        KEYSTONE_BASE_SHIPPING = 11.0\n",
    "        KEYSTONE_LTL_SHIPPING = 175.0\n",
    "        inv = pd.read_csv('inventory/keystone.csv', low_memory=False)\n",
    "        # Since inventory file open, manage duplicate part # issue in Keystone by matching with UPC\n",
    "        inv['PartNumber'] = inv['PartNumber'].str.replace('=','').str.replace('\"','')\n",
    "        ##inv['KeystoneShipping'] = (inv['UPS_Ground_Assessorial'] + KEYSTONE_BASE_SHIPPING).fillna(0)\n",
    "        inv['KeystoneShipping'] = (KEYSTONE_BASE_SHIPPING)\n",
    "        inv.loc[inv['UPSable']==False, 'KeystoneShipping'] = KEYSTONE_LTL_SHIPPING\n",
    "        inv = inv.sort_values('KeystoneShipping', ascending=False).drop_duplicates(subset=['VendorCode','PartNumber'])\n",
    "        df = df.merge(inv[['VendorCode','PartNumber','KeystoneShipping']], \n",
    "                      how='left', left_on=['LC','Part Number'], right_on=['VendorCode','PartNumber'])\n",
    "        df['Shipping'] = df['KeystoneShipping']\n",
    "        df['ShipMkup'] = 1\n",
    "        df.drop(columns=['VendorCode', 'PartNumber', 'KeystoneShipping'], inplace=True)\n",
    "\n",
    "    elif warehouse=='6': # Burco Mirrors\n",
    "        df['Shipping'] = 8 # estimate / avg        \n",
    "        df['ShipMkup'] = 1\n",
    "    elif warehouse=='A': # APW\n",
    "        df['Shipping'] = get_historical_shipping_estimates(df, warehouse)\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "    elif warehouse=='2': # OE Wheels\n",
    "        df['Shipping'] = get_historical_shipping_estimates(df, warehouse)\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "        \n",
    "    elif warehouse=='5': # KSI Trading\n",
    "        dgt = pd.read_csv('DGTire Shipping.csv', low_memory=False, dtype={'Part Number':'str'})   #Reading the DGTire Shipping Estimates from disk\n",
    "        dgt['WD'] = '5'\n",
    "        dgt['MasterLC'] = 150\n",
    "        df = df.merge(dgt, how='inner', on=['WD', 'MasterLC', 'Part Number'], suffixes=('','_dgt'))   #Merging the pw-report dataframe with the Shipping Estimates dataframe\n",
    "        df.loc[~df['ship_cost'].isna(), 'Shipping'] = df['ship_cost']   #Moving Shipping cost estimate to the Shipping column\n",
    "        df.drop(columns=['Brand', 'ship_cost', 'shipping_est', 'FET'], inplace=True)   #Removing additional columns\n",
    "        del dgt   #Deleting the wtd dataframe as it is no longer needed\n",
    "    \n",
    "    elif warehouse=='7': # WTD Arroyo\n",
    "        wtd = pd.read_csv('WTD Shipping.csv', low_memory=False)   #Reading the WTD Shipping Estimates from disk\n",
    "        wtd['WD'] = '7'\n",
    "        wtd['MasterLC'] = 153\n",
    "        df = df.merge(wtd, how='inner', on=['WD', 'MasterLC', 'Part Number'], suffixes=('','_wtd'))   #Merging the pw-report dataframe with the Shipping Estimates dataframe\n",
    "        df.loc[~df['ship_cost'].isna(), 'Shipping'] = df['ship_cost']   #Moving Shipping cost estimate to the Shipping column\n",
    "        df.drop(columns=['Brand', 'ship_cost'], inplace=True)   #Removing additional columns\n",
    "        del wtd   #Deleting the wtd dataframe as it is no longer needed\n",
    "        \n",
    "    elif warehouse=='8': # Race Sport Lighting\n",
    "        ##df['Shipping'] = df['Weight'].map(lambda w: 15 if (pd.isna(w) or w >= 1) else 6)\n",
    "        ##df['ShipMkup'] = DEFAULT_SHIP_MARKUP        \n",
    "        df['Shipping'] = 23.5\n",
    "        df['ShipMkup'] = 1        \n",
    "    elif warehouse=='9': # Sunbelt APG\n",
    "        df['Shipping'] = 22\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "    \n",
    "    elif warehouse=='N': # NPW\n",
    "        df['Shipping'] = 6\n",
    "        df.loc[df['MinPrice']>15, 'Shipping'] = 10\n",
    "        df.loc[df['MinPrice']>30, 'Shipping'] = 15\n",
    "        df.loc[df['MinPrice']>50, 'Shipping'] = 20\n",
    "        df.loc[df['MinPrice']>80, 'Shipping'] = 35\n",
    "        df.loc[df['MinPrice']>150, 'Shipping'] = 60\n",
    "        df.loc[df['MinPrice']>300, 'Shipping'] = 100\n",
    "        df.loc[df['MinPrice']>500, 'Shipping'] = 200\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "    \n",
    "    elif warehouse=='O': # Tonsa\n",
    "        #df['Shipping'] = get_historical_shipping_estimates(df, warehouse)\n",
    "        #df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "        df_cols = df.columns\n",
    "        tonsa = pd.read_csv('Tonsa Shipping.csv', low_memory=False)\n",
    "        tonsa['Part Number'] = tonsa['Tonsa#'].map(lambda s: ''.join(filter(str.isalnum, s)))   #Removing Punctuation from Part Number\n",
    "        tonsa['WD'] = 'O'        \n",
    "        df = df.merge(tonsa, how='left', left_on=['WD', 'LC', 'Part Number'], right_on=['WD', 'Mfg', 'Part Number'], suffixes=('','_tonsa'))\n",
    "        df.loc[~df['ship_cost'].isna(), 'Shipping'] = df.loc[~df['ship_cost'].isna(), 'ship_cost']\n",
    "        df = df[df_cols]\n",
    "        del tonsa\n",
    "        del df_cols\n",
    "\n",
    "        \n",
    "    elif warehouse=='Y': # MotorState\n",
    "        df.loc[(df['item_cost'] <= 39.99), 'Shipping'] = 13\n",
    "        df.loc[((df['item_cost'] > 39.99) & (df['item_cost'] <= 99.99)), 'Shipping'] = 12\n",
    "        df.loc[(df['item_cost'] >= 100), 'Shipping'] = 11\n",
    "        df['ShipMkup'] = 1        \n",
    "    \n",
    "    elif warehouse=='Z': # SimpleTire\n",
    "        df['Shipping'] = 0        \n",
    "        df['ShipMkup'] = 1    \n",
    "        df_cols = df.columns\n",
    "        st_FET = pd.read_csv('SimpleTire FET.csv', dtype={'PRODUCT_ID':str})   #Reading the FET file\n",
    "        st_FET['WD'] = 'Z'\n",
    "        df = df.merge(st_FET, left_on=['Part Number', 'WD'], right_on=['PRODUCT_ID', 'WD'], how='left')   #Merging with dataframe\n",
    "        df.loc[~df['FET'].isna(), 'Shipping'] = df.loc[~df['FET'].isna(), 'FET']   #Copying FET as Shipping where applicable\n",
    "        df = df[df_cols]   #Removing extra columns\n",
    "        del df_cols, st_FET   #Deleting the dataframes which are no longer needed\n",
    "    \n",
    "    elif warehouse=='F':   #Keystone Crash (LKQ)\n",
    "        LKQ = pd.read_csv('LKQ Shipping.csv', low_memory=False, usecols=['Product', 'Ship Cost'])   #Reading LKQ Ship Cost file from disk\n",
    "        LKQ['WD'] = 'F'   \n",
    "        df = df.merge(LKQ, how='left', left_on=['WD', 'Part Number'], right_on=['WD', 'Product'])   #Merging Price Weight Report with LKQ Ship Cost file\n",
    "        df.loc[df['WD'] == 'F', 'Shipping'] = df.loc[df['WD'] == 'F', 'Ship Cost']\n",
    "        df.drop (columns=['Product', 'Ship Cost'], inplace=True)\n",
    "        del LKQ   #Deleting the LKQ Ship cost file from memory as it is no longer needed\n",
    "        \n",
    "    elif warehouse=='U': #Turn 14\n",
    "        df['Shipping'] = 13\n",
    "        \n",
    "        #Using log function for prices above $5\n",
    "        # a = 30.090392996378466\n",
    "        # b = 1.5403837582252213\n",
    "        # #a*math.log(i*b)\n",
    "        # df['lship'] = df['MinPrice'].apply(lambda x:a*math.log(x*b) if x>5 else None )\n",
    "        # df.loc[df['MinPrice']>5, 'Shipping'] = df['lship']\n",
    "        # df.drop(columns=['lship'], inplace=True)\n",
    "        \n",
    "        df.loc[df['MinPrice']>15, 'Shipping'] = 15\n",
    "        df.loc[df['MinPrice']>30, 'Shipping'] = 25\n",
    "        df.loc[df['MinPrice']>50, 'Shipping'] = 20\n",
    "        df.loc[df['MinPrice']>80, 'Shipping'] = 35\n",
    "        df.loc[df['MinPrice']>150, 'Shipping'] = 60\n",
    "        df.loc[df['MinPrice']>300, 'Shipping'] = 100\n",
    "        df.loc[df['MinPrice']>500, 'Shipping'] = 180\n",
    "        \n",
    "        #Additional adjustments\n",
    "        t14 = pd.read_csv('inventory/t14.csv', low_memory=False)  #Reading the Turn14 inventory file\n",
    "        t14.columns.values[0] = 'sku'\n",
    "        t14['part'] = t14.iloc[:,0].str.split('|', expand=True).iloc[:,1]   #Extracting part number from sku\n",
    "        t14 = t14[['line_code', 'part', 'ltl_freight_required']].drop_duplicates()   #Removing extra columns\n",
    "        df = df.merge(t14,  how='left', left_on=['LC', 'Part Number'], right_on=['line_code', 'part'])\n",
    "        df.loc[df['ltl_freight_required']==1, 'Shipping'] = 175\n",
    "        df.drop(columns='ltl_freight_required', inplace=True)\n",
    "        del t14\n",
    "        \n",
    "        t14_DS = pd.read_csv('Turn 14 DS Detail.csv', low_memory=False)\n",
    "        t14_DS['IPartNumber'] = t14_DS['IPartNumber'].str.lower()\n",
    "        t14_DS['IPartNumber'] = t14_DS['IPartNumber'].str.replace(' ', '', regex=False)   #The Part in pw report doesn't have spaces\n",
    "        df['t14_pn'] = df['LC'].str.lower() + df['Part Number'].str.lower()\n",
    "\n",
    "        #Data adjustments for merge\n",
    "        df.loc[df['LC'] == 'KW', 't14_pn'] = 'kws' + df.loc[df['LC'] == 'KW', 'Part Number'].str.lower()   #LC KW is changed to KWS \n",
    "        df.loc[df['LC'] == 'ATI', 't14_pn'] = 'app' + df.loc[df['LC'] == 'ATI', 'Part Number'].str.lower()   #ATI is different in file\n",
    "        df.loc[df['LC'] == 'KPT', 't14_pn'] = 'kot' + df.loc[df['LC'] == 'KPT', 'Part Number'].str.lower()   #LC KPT is changed to KOT\n",
    "        df.loc[((df['LC'] == 'NEX') & (df['Part Number'].str.startswith('SNO'))), 't14_pn'] = 'sno'+ df.loc[((df['LC'] == 'NEX') & (df['Part Number'].str.startswith('SNO'))), 'Part Number'].str.lower()\n",
    "        df.loc[((df['LC'] == 'PER') & (df['Part Number'].str.startswith('AMP'))), 't14_pn'] = 'alt'+ df.loc[((df['LC'] == 'PER') & (df['Part Number'].str.startswith('AMP'))), 'Part Number'].str.lower()\n",
    "        df.loc[((df['LC'].isna()) & (df['MasterLC']== 821)), 't14_pn'] = df.loc[((df['LC'].isna()) & (df['MasterLC']== 821)), 'Part Number'].str.lower()\n",
    "        df.loc[((df['LC'].isna()) & (df['MasterLC']== 119)), 't14_pn'] = 'per' + df.loc[((df['LC'].isna()) & (df['MasterLC']== 119)), 'Part Number'].str.lower()\n",
    "\n",
    "        df = df.merge( t14_DS, how='left', left_on='t14_pn', right_on='IPartNumber', suffixes=('', '_y'))\n",
    "\n",
    "        df.loc[~df['DSFee'].isnull(), 'Shipping_Add'] = df.loc[~df['DSFee'].isnull(), 'Shipping_Add'] + df.loc[~df['DSFee'].isnull(), 'DSFee']   #Adding additional Drop Ship Fee as additional Shipping\n",
    "        df = df.query(\"DropShip != 'never' \")   #Excluding the items which are never allowed to be drop shipped\n",
    "        df.drop(columns=['t14_pn', 'DSFee', 'IPartNumber', 'DropShip', 'Weight_y'], inplace=True)\n",
    "        del t14_DS\n",
    "    \n",
    "    elif warehouse=='3':        \n",
    "        wp = pd.read_csv('WP_Shipping.csv', low_memory=False)\n",
    "        wp['WD'] = '3'\n",
    "        wp['Part Number'] = wp['Part Number'].map(lambda s: ''.join(filter(str.isalnum, s)))   #Removing Punctuation from sku\n",
    "        wp.drop_duplicates(subset=['Part Number'], inplace=True)\n",
    "        df = df.merge(wp, how='left', left_on=['LC', 'WD', 'Part Number'], right_on=['MasterLC', 'WD', 'Part Number'], suffixes=('','_wp'))\n",
    "        df.loc[~df['ship_cost'].isnull(), 'Shipping'] = df.loc[~df['ship_cost'].isnull(), 'ship_cost']\n",
    "\n",
    "        df.drop(columns=['ship_cost', 'Category', 'LC_wp', 'MasterLC_wp'], inplace=True)\n",
    "        del wp\n",
    "        \n",
    "    elif warehouse=='4':\n",
    "        df.loc[df['WD']=='4', 'Shipping'] = 0\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    df['ShipMkup'] = df['ShipMkup'].fillna(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac403e15-8384-49cf-bb3f-d96984a729f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97b065b6-4ed9-4fb1-89df-db01a9aac65a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n",
      "Proportion of parts missing shipping: 0.0\n",
      "1\n",
      "Proportion of parts missing shipping: 0.0\n",
      "2\n",
      "Proportion of parts missing shipping: 0.0\n",
      "3\n",
      "Proportion of parts missing shipping: 0.29829798730244494\n",
      "4\n",
      "Proportion of parts missing shipping: 0.0\n",
      "5\n",
      "Proportion of parts missing shipping: 0.0\n",
      "6\n",
      "Proportion of parts missing shipping: 0.0\n",
      "7\n",
      "Proportion of parts missing shipping: 0.0018673306064378445\n",
      "8\n",
      "Proportion of parts missing shipping: 0.0\n",
      "9\n",
      "Proportion of parts missing shipping: 0.0\n",
      "K\n",
      "Proportion of parts missing shipping: 0.12040887962303086\n",
      "F\n",
      "Proportion of parts missing shipping: 0.007897696998401515\n",
      "Y\n",
      "Proportion of parts missing shipping: 0.0\n",
      "N\n",
      "Proportion of parts missing shipping: 0.0\n",
      "P\n",
      "RMSE: 5.532038491377326\n",
      "Proportion of parts missing shipping: 0.0\n",
      "J\n",
      "Proportion of parts missing shipping: 0.012560261149663835\n",
      "Z\n",
      "Proportion of parts missing shipping: 0.0\n",
      "O\n",
      "Proportion of parts missing shipping: 0.14407512665266278\n",
      "U\n",
      "Proportion of parts missing shipping: 0.0\n"
     ]
    }
   ],
   "source": [
    "#new\n",
    "dfs = []\n",
    "for warehouse in pw['WD'].unique().tolist():\n",
    "    wdf = pw[pw['WD']==warehouse].copy()\n",
    "    wdfp = calculate_warehouse_shipping(wdf, warehouse)\n",
    "                                                                           \n",
    "    print('Proportion of parts missing shipping:', wdfp['Shipping'].isna().mean())\n",
    "    dfs.append(wdfp)\n",
    "    \n",
    "pw = pd.concat(dfs, ignore_index=True)\n",
    "#pw['ShipMkup'] = pw['ShipMkup'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76619bb5-ab27-4af9-bc67-ff79adc96ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0835d3ed-4bb5-4927-8124-6fdcc6147712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting datasets from memory which are no longer needed\n",
    "del shipping_data, price_file_shipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-female",
   "metadata": {},
   "source": [
    "#### Set inventory constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "intended-capability",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pw.loc[:, ['MinQty','MaxQty']] = 3, 12   ## Changed Min to two from three\n",
    "\n",
    "pw.loc[pw['MasterLC']==145, ['MinQty','MaxQty']] = 10, 24   ## Updating min and Max Qty for Simple tire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-mumbai",
   "metadata": {},
   "source": [
    "#### Set price file defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "arbitrary-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw['ListMkup'] = .65\n",
    "pw['SourceQty'] = None\n",
    "pw['Source'] = None\n",
    "pw['BundleSK'] = None\n",
    "pw['Carrier'] = 'FedEx'\n",
    "pw['Service'] = 'GroundHD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19661ab1-69de-4d12-9aae-44c470a98614",
   "metadata": {},
   "source": [
    "Set OP-Lowest(Y) and VND-Lowest(Y) to \"N\" for Dorman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8328495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[(pw['CS-SKU-NP'].str[0]=='D'), ['OP-Lowest(Y)','VND-Lowest(Y)'] ] = \"N\", \"N\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-coordinate",
   "metadata": {},
   "source": [
    "## Quick fix for shipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-staff",
   "metadata": {},
   "source": [
    "Where shipping is current zero for Tonsa, set it to 20. And 15 for Sunbelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "skilled-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[((pw['Shipping'] == 0) & (pw['CS-SKU-NP'].str[0] == 'O')), 'Shipping'] = 20\n",
    "pw.loc[((pw['Shipping'] == 0) & (pw['CS-SKU-NP'].str[0] == '9')), 'Shipping'] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-mongolia",
   "metadata": {},
   "source": [
    "1.5x ~Tonsa, Sunbelt~, and Parts Auth shipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fifty-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pw.loc[(pw['CS-SKU-NP'].str[0].isin(['O','9','P'])), 'Shipping'] *= 1.5\n",
    "pw.loc[(pw['CS-SKU-NP'].str[0]=='P'), 'Shipping'] *= 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-aluminum",
   "metadata": {},
   "source": [
    "Double Eagle Eye Shipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "alleged-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[(pw['CS-SKU-NP'].str[:4]=='P754'), 'Shipping'] *= 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-exemption",
   "metadata": {},
   "source": [
    "Bumper is expensive to ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "treated-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='429|6448-0006', 'Shipping'] = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-government",
   "metadata": {},
   "source": [
    "Another expensive shipping update from order: PSA669874628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "proud-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='551|S6585B', 'Shipping'] = 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e8265",
   "metadata": {},
   "source": [
    "Expensive shipping for part from 12/20/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c24bfec-0351-4002-b347-1c2adf6dbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P576|3292', 'Shipping'] = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37bf81",
   "metadata": {},
   "source": [
    "Expensive shipping for part from 12/27/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "022907cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P550|290073', 'Shipping'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d6fb22b-e3c0-4093-ad08-dd6d2f1212e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P308|55621', 'Shipping'] = 161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6391f25-f8ac-4346-a902-6579aa63bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P557|277504', 'Shipping'] = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f247128-2913-439f-9824-37112a1dbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P643|ESK5752', 'Shipping'] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8271b50-33b8-4f5b-94e6-512cd7e64d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P551|40722A', 'Shipping'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f804e63b-1bc7-48e8-819b-87c2488f2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='N643|AR8265XPR', 'PackQty'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8bc3580-3b4f-4d21-905e-119dd8993309",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P123|33660', 'PackQty'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f2488-c540-4666-844d-75f230184d68",
   "metadata": {},
   "source": [
    "PA Shipping Costs from Umer analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bd9a1b1-5a7e-4fa2-9b00-95a22b693168",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_shipping_data = pd.read_csv('PA Shipping Costs.csv', low_memory=False)\n",
    "pa_shipping_data['WD'] = 'P'\n",
    "\n",
    "pa_shipping_data = pa_shipping_data.sort_values(by='Row Labels', ascending=False)\n",
    "pa_shipping_data.drop_duplicates(subset='Row Labels', keep=\"first\")\n",
    "\n",
    "pw = pw.merge(pa_shipping_data, how='left', left_on=['MasterSKU', 'WD'], right_on=['Row Labels', 'WD'])\n",
    "pw.loc[pw['Final Shipping Cost'] > 0, 'Shipping'] = pw['Final Shipping Cost']\n",
    "pw = pw[pw_cols]   #Restoring columns before this Merge\n",
    "\n",
    "del pa_shipping_data   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22038c91-f4c2-4b77-ab00-0825628631f4",
   "metadata": {},
   "source": [
    "NPW Pack Corrections and updates from Abdullah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "206c1bd7-919b-43b9-8575-d292c5264461",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='N223|97469', 'PackQty'] = 10\n",
    "pw.loc[pw['CS-SKU-NP']=='N178|VL1093', 'PackQty'] = 4\n",
    "pw.loc[pw['CS-SKU-NP']=='N223|98288', 'PackQty'] = 10 #PackQty issue - 20-May-2022\n",
    "pw.loc[pw['CS-SKU-NP']== 'N114|3025', 'PackQty'] = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "062e9bc2-54ca-441d-a2d1-8ba108169757",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw['MasterSKUn'] = pw['CS-SKU-NP'].str[1:]\n",
    "shipping_fix = pd.read_csv('Shipping Corrections.csv', low_memory=False, encoding='unicode_escape')\n",
    "shipping_fix = shipping_fix.drop_duplicates(['WD', 'SKU'])\n",
    "\n",
    "pw = pw.merge(shipping_fix, how='left', left_on=['MasterSKUn', 'WD'], right_on=['SKU', 'WD'])\n",
    "pw.loc[pw['Final Shipping Cost'] > 0, 'Shipping'] = pw['Final Shipping Cost']\n",
    "pw.loc[pw['Shipping_Add']!=0.0, 'Shipping'] = pw.loc[pw['Shipping_Add']!=0.0, 'Shipping'] + pw.loc[pw['Shipping_Add']!=0.0, 'Shipping_Add']\n",
    "pw = pw[pw_cols]\n",
    "\n",
    "del shipping_fix   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2145f776-fdaa-4320-8388-fc6a5d532e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shipping = pd.read_csv('newshipcosts.csv', low_memory=False)\n",
    "\n",
    "pw = pw.merge(new_shipping, how='left', left_on=['MasterSKU'], right_on=['CS_SKU'])\n",
    "pw.loc[pw['NewShipCost'] > 0, 'Shipping'] = pw['NewShipCost']\n",
    "pw = pw[pw_cols]\n",
    "\n",
    "del new_shipping   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-convenience",
   "metadata": {},
   "source": [
    "Set MinQty really high for First Stop Brakes (Dorman line) to avoid actually selling any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sunrise-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stop_brakes = pd.read_excel('8-2 Change 56 Brake Dropship and Stocking.xlsx', \n",
    "                                  skiprows=2, sheet_name='Dropship Price').rename(columns={'MATERIAL':'pn'})['pn']\n",
    "pw.loc[(pw['WD']=='D') & pw['Part Number'].isin(first_stop_brakes), ['MinQty','MaxQty']] = 100, 100\n",
    "\n",
    "del first_stop_brakes   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347dcb1",
   "metadata": {},
   "source": [
    "Handle RSL skus for MAP > Calculated Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2751520c-6d4f-48c5-8d04-3634d3727be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading RSL Inventory and Removing Duplicate SKUs\n",
    "rsl_inventory = pd.read_csv('inventory/rsl.csv', low_memory=False)\n",
    "rsl_inventory.sort_values(by='MAP', ascending=False, inplace=True)\n",
    "rsl_inventory.drop_duplicates(subset=['SKU'], inplace=True, keep='first')\n",
    "rsl_inventory['CS-SKU-NP'] = '8329|' + rsl_inventory['SKU']\n",
    "rsl_inventory.set_index('CS-SKU-NP', inplace=True)\n",
    "\n",
    "\n",
    "pw = pw.join(other = rsl_inventory, on='CS-SKU-NP', rsuffix='_rsl')\n",
    "pw['tmp_mkup'] = pw['item_cost'] / ( (1 + 0.05) * (pw['item_cost'] + pw['Shipping']) / (1 - 0.15) - pw['Shipping'])\n",
    "pw['tmp_price'] = pw['item_cost']/pw['tmp_mkup'] + pw['Shipping']/pw['ShipMkup']\n",
    "\n",
    "pw.loc[pw['tmp_price']<pw['MAP_rsl'], ['MinPrice', 'item_cost']] = pw.loc[pw['tmp_price']<pw['MAP_rsl'], 'MAP_rsl']\n",
    "pw.loc[pw['tmp_price']<pw['MAP_rsl'], ['Shipping', 'ShipMkup']] = 0\n",
    "\n",
    "pw = pw[pw_cols]   #Restoring columns\n",
    "del rsl_inventory   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "063bb6f9-dcd9-4059-bc3d-f33be1383962",
   "metadata": {},
   "outputs": [],
   "source": [
    "motorstate = pd.read_csv('inventory/motorstate.csv', low_memory=False)\n",
    "#motorstate['PartNumber'] = motorstate.PartNumber.str[3:]\n",
    "motorstate['PartNumber'] = 'Y' + motorstate['PartNumber']\n",
    "#pw = pw.merge(motorstate, how='left', left_on=['Part Number', 'WD'], right_on=['PartNumber', 'WD'])\n",
    "\n",
    "pw['tkey'] = pw['WD'] + pw['LC'] + pw['Part Number']\n",
    "\n",
    "pw = pw.merge(motorstate, how='left', left_on=['tkey'], right_on=['PartNumber'])\n",
    "pw.drop(columns='tkey', inplace = True)\n",
    "\n",
    "del motorstate   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c06c807d-b4e6-41a7-b23f-111ed7dd638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.drop(pw.loc[pw['AirRestricted'] == 'YES'].index, inplace=True)\n",
    "pw.drop(pw.loc[pw['TruckFrtOnly'] == 'YES'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da908c40-bb20-4fd4-b877-07f4d97790d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pw.drop(pw.loc[(pw['WD'] == 'Y') & (pw['MasterLC'] != 261)].index, inplace=True)\n",
    "pw = pw [pw_cols]   #Restoring Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec6179-f238-4d07-a1b5-11d7fc313a93",
   "metadata": {},
   "source": [
    "NPW Min Order Qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f5d860b-7ff6-4b15-af98-ecaea9f636ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnpw_packqty = pd.read_csv('NPW PackQty.csv', low_memory=False)\\nnpw_packqty['WD'] = 'N'\\nnpw_packqty.set_index(['WD', 'Line Code', 'Part Number'], inplace=True)\\nnpw_packqty\\n\\npw1 = pw.join(other=npw_packqty, on=['WD', 'LC', 'Part Number'], how='left')\\npw1.loc[~pw1['NPW_PackQty'].isna()]\\n\\nf = (pw1['WD']=='N') & (~pw1['NPW_PackQty'].isna())   #Creating the filter for items to be updated\\npw1.loc[f & (pw1['PackQty']!=pw1['NPW_PackQty'])].to_excel('NPW PackQty Not Matching.xlsx')\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "npw_packqty = pd.read_csv('NPW PackQty.csv', low_memory=False)\n",
    "npw_packqty['WD'] = 'N'\n",
    "npw_packqty.set_index(['WD', 'Line Code', 'Part Number'], inplace=True)\n",
    "npw_packqty\n",
    "\n",
    "pw1 = pw.join(other=npw_packqty, on=['WD', 'LC', 'Part Number'], how='left')\n",
    "pw1.loc[~pw1['NPW_PackQty'].isna()]\n",
    "\n",
    "f = (pw1['WD']=='N') & (~pw1['NPW_PackQty'].isna())   #Creating the filter for items to be updated\n",
    "pw1.loc[f & (pw1['PackQty']!=pw1['NPW_PackQty'])].to_excel('NPW PackQty Not Matching.xlsx')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e62bab",
   "metadata": {},
   "source": [
    "OE wheels price MAP fix 11/09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "549e567a-050b-4053-849e-71e4eab0695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##oe_wheel_update = pd.read_excel('OE wheel price increase11-9.xlsx', sheet_name='Sheet1')\n",
    "oe_wheel_update = pd.read_excel('OE Wheel Shipping.xlsx', sheet_name='Sheet1', dtype={'UPC':str})\n",
    "oe_wheel_update.drop_duplicates(subset=['UPC'], inplace=True, keep='last')\n",
    "\n",
    "oe_wheel_update['CS-SKU-NP'] = '2387|' + oe_wheel_update['UPC']\n",
    "oe_wheel_update.set_index('CS-SKU-NP', inplace=True)\n",
    "\n",
    "pw = pw.join(other = oe_wheel_update, on='CS-SKU-NP', rsuffix='_oe')\n",
    "pw.loc[~pw['Shipping (Est)'].isna(), 'Shipping'] = pw.loc[~pw['Shipping (Est)'].isna(), 'Shipping (Est)']\n",
    "pw = pw[pw_cols]   #Restoring Columns\n",
    "del oe_wheel_update   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00826ada-212a-417f-a4e3-c2ad7a66dfee",
   "metadata": {},
   "source": [
    "PA packQty Correction from inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55b23ce3-2c1e-43fb-ae61-b8b998ffb8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_inv = pd.read_csv('inventory/pa.csv', low_memory=False, encoding= 'unicode_escape')\n",
    "pa_inv['Part'] = pa_inv['Part'].str.replace('-', '', regex=False )\n",
    "pa_inv['Part'] = pa_inv['Part'].str.replace('.', '', regex=False )\n",
    "pa_inv['Part'] = pa_inv['Part'].str.replace('/', '', regex=False )\n",
    "pa_inv['Total_Stock'] = pa_inv[['BxStock', 'ByStock', 'NYStock', 'DCStock', 'AZStock', 'CAStock', 'GAStock']].sum(axis=1)\n",
    "\n",
    "\n",
    "pw = pw.merge(pa_inv, how='left', left_on=['Part Number', 'LC'], right_on=['Part', 'Line'], suffixes=('', 'y'))\n",
    "pw.loc[~pw['Packs'].isna(), 'PackQty'] = pw.loc[~pw['Packs'].isna(), 'Packs']\n",
    "\n",
    "#Manual Fetching the Price and Quantity from PA inventory\n",
    "#pw.loc[~pw['Price'].isna(), 'MinPrice'] = pw.loc[~pw['Price'].isna(), 'Price']\n",
    "#pw.loc[~pw['Price'].isna(), 'item_cost'] = pw.loc[~pw['Price'].isna(), 'Price']\n",
    "#pw.loc[~pw['Total_Stock'].isna(), 'Qty'] = pw.loc[~pw['Total_Stock'].isna(), 'Total_Stock']\n",
    "\n",
    "pw = pw[pw_cols]   #Restoring Columns\n",
    "\n",
    "del pa_inv   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945265d0-08d5-43fc-a9cc-0a594d729723",
   "metadata": {
    "tags": []
   },
   "source": [
    "Force shipping cost and ship markup for RSL Skus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8dc9cad7-32d8-46eb-b29c-3cf2774313e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[(pw['WD'] == '8') & (pw['Shipping'] == 0), ['Shipping', 'ShipMkup']] = 18, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e91d9c06-a545-4c3c-b608-060c19efd7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Speicfic SKU(s) from Price Files\n",
    "#SKU 1 P650|W41029 : Marion said on 30th September its Refunded 6 times so remove from price file \n",
    "pw = pw.loc[pw['CS-SKU-NP']!= \"P650|W41029\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bda58ba2-292b-4b88-adf9-ae7f4f1c2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preventing Race Sport Lightning items from any other warehouse\n",
    "pw.drop(index=pw.query(\"MasterLC == 329 and WD != '8' \").index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44161170-2a8f-4500-89e4-52cf77bee5e5",
   "metadata": {},
   "source": [
    "Exclude CoverCraft Temporarily (We are on their don't sell list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "901df2a7-6608-4180-b54c-90b983854d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = pw.loc[~pw['MasterLC'].isin ([571])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55dc229-9d3c-4f7e-bb8c-291b61b68ff1",
   "metadata": {},
   "source": [
    "Route Dorman Drive Shafts and Dorman Pack Items to Dorman only (remove it from any other warehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ecec1ce-d60a-4496-a0ee-111a7a03e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOI = pd.read_csv('Dorman Only Items.csv', low_memory=False, usecols=['MasterSKU']).drop_duplicates()   #Loading the List of items which should only be routed to Dorman\n",
    "pw = pw.merge(right = DOI, how='left', on='MasterSKU', indicator=True)   #Merging the Dorman list to the Price Weight Report\n",
    "pw.drop(pw.query(\"_merge == 'both' and WD!='D'\").index, inplace=True)   #Dropping the DormanDriveShafts and Dorman Pack Items from any other Warehouse Except Dorman\n",
    "pw.drop(columns = '_merge', inplace=True)   #Drop the merging indicator column\n",
    "del DOI   #Delete the list as it is no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-pioneer",
   "metadata": {},
   "source": [
    "#### Calculate markups and format/write price files, by channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3fbeb-7a53-4100-9ac5-595beb731929",
   "metadata": {},
   "source": [
    "Load Pack SKUs Data from file (to be removed from Price Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c083d840-1b88-452c-a894-75e90f5997e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "packskus = pd.read_csv('Pack SKUs removed.csv', index_col='MasterSKU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef152355-87a3-4ab2-9592-76655d9adba5",
   "metadata": {},
   "source": [
    "NPW Temporary min Ship Estimate set at 12 (for safe transition to non-flatrate shipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbfcd70e-2c30-47b6-af88-a16cb31a01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[(pw['WD']=='N') & (pw['Shipping']<12), 'Shipping'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68efe747-174b-471d-8483-d2073ee14ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pd.read_excel('configuration.xlsx', sheet_name=None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a497ac26-546c-4190-868a-f3788596173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf['Channels'].set_index(keys = conf['Channels']['id'], inplace=True, verify_integrity=True, drop=False)   #Set the channel id as index\n",
    "conf['Channels']['Catalog Fee'] = conf['Channels']['Catalog Fee'].fillna(0)   #Filling null Catalog Fee as 0\n",
    "conf['Channels'] = conf['Channels'].to_dict('index')   #Converting to an easily searchable dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d41ce2-5289-4a85-9841-35b0470973f9",
   "metadata": {},
   "source": [
    "LKQ Remove Paint bumper items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74f2c11c-61df-4b40-8120-98396ac68ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lkq_bp = pd.read_csv('LKQ Bumper Paint Issue.csv', low_memory=False)\n",
    "pw = pw.merge(lkq_bp, left_on='MasterSKU', right_on='sku', how='left', indicator=True)\n",
    "di = pw.query(\"_merge=='both'\").index\n",
    "pw.drop(index=di, inplace=True)\n",
    "pw.drop(columns='_merge', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd8c32af-8402-44d1-aac4-07b364c3ceb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set the values from configuration table\n",
    "def set_conf_val(dset, c_tbl, dsetcol, vcol): \n",
    "    for _, crow in c_tbl.iterrows():   #iterating over the configuration table entries\n",
    "        crow = crow[lambda x: ~x.isna()] ###Excluding null columns\n",
    "        \n",
    "        if vcol in crow:   #if the Value column has a value in configuration\n",
    "\n",
    "            def fil_fn(df):\n",
    "                Trues = np.repeat(a=[True], repeats=len(df))            \n",
    "                if 'Warehouse' in crow:\n",
    "                    wd = df['WD'] == crow['Warehouse']\n",
    "                else:                \n",
    "                    wd = Trues.copy()\n",
    "\n",
    "                if 'MasterLC' in crow:\n",
    "                    lc = df['MasterLC'] == crow['MasterLC']\n",
    "                else:                \n",
    "                    lc = Trues.copy()                \n",
    "\n",
    "                if 'Min Cost' in crow:\n",
    "                    min_c = df['MinPrice'] >= crow['Min Cost']\n",
    "                else:                \n",
    "                    min_c = Trues.copy()\n",
    "\n",
    "                if 'Max Cost' in crow:\n",
    "                    max_c = df['MinPrice'] <= crow['Max Cost']\n",
    "                else:                \n",
    "                    max_c = Trues.copy()\n",
    "\n",
    "                return ((wd) & (lc) & (min_c) & (max_c))\n",
    "\n",
    "\n",
    "            dset.loc[fil_fn, dsetcol] = crow[vcol]   #Setting the dataset column with the provided value\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5bf25ef-59f1-433e-8cf7-f8a232a52f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Exclude Warehouses and Brands from configuration file\n",
    "def exclude_conf(dset, c_tbl):    \n",
    "    for _, crow in c_tbl.iterrows():   # iterating over the configuration table entries\n",
    "        crow = crow[lambda x: ~x.isna()] # Excluding null columns\n",
    "\n",
    "        def fil_fn(df):\n",
    "            Trues = np.repeat(a=[True], repeats=len(df))            \n",
    "            if 'Warehouse' in crow:\n",
    "                wd = df['WD'] == crow['Warehouse']\n",
    "            else:                \n",
    "                wd = Trues.copy()\n",
    "\n",
    "            if 'MasterLC' in crow:\n",
    "                lc = df['MasterLC'] == crow['MasterLC']\n",
    "            else:                \n",
    "                lc = Trues.copy()                \n",
    "\n",
    "            if 'Min Cost' in crow:\n",
    "                min_c = df['MinPrice'] >= crow['Min Cost']\n",
    "            else:                \n",
    "                min_c = Trues.copy()\n",
    "\n",
    "            if 'Max Cost' in crow:\n",
    "                max_c = df['MinPrice'] <= crow['Max Cost']\n",
    "            else:                \n",
    "                max_c = Trues.copy()\n",
    "\n",
    "            return ((wd) & (lc) & (min_c) & (max_c))\n",
    "\n",
    "        dset.drop( index= dset.loc[fil_fn].index, inplace=True)   #Excluding the items in the filter list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5336ae41-a9d3-4639-a511-b789e8d6b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_channel(channel): \n",
    "    \n",
    "    pf = pw.copy()\n",
    "    pf['TP'] = 0.999    #Creating Target Profit column with huge placeholder value\n",
    "    pf['STP'] = 0.999    #Creating Shipping Target Profit column with huge placeholder value\n",
    "    pf['CAdd'] = 0;   #Creating Cost addition/adjustment column  with 0 default value\n",
    "    \n",
    "    c = conf['Channels'][channel['id']]   #Reading Channel info of corresponding channel from the configuration Excel\n",
    "    channel_fee = c['Fee']     #Reading fee of the channel\n",
    "    \n",
    "    #Setting values from Configuration file\n",
    "    set_conf_val(pf, conf['Target Profit'].query(\"Channel ==  @c['id'] or Channel.isna()\"), 'TP', 'Value')   #Setting Target Profit as per configuration\n",
    "    set_conf_val(pf, conf['Shipping Target Profit'].query(\"Channel ==  @c['id'] or Channel.isna()\"), 'STP', 'Value')   #Setting Shipping Target Profit as per configuration\n",
    "    set_conf_val(pf, conf['Quantities'].query(\"Channel ==  @c['id'] or Channel.isna()\"), 'MinQty', 'MinQty')   #Setting MinQty as per configuration\n",
    "    set_conf_val(pf, conf['Quantities'].query(\"Channel ==  @c['id'] or Channel.isna()\"), 'MaxQty', 'MaxQty')   #Setting MaxQty as per configuration\n",
    "    set_conf_val(pf, conf['Cost Addition'].query(\"Channel == @c['id'] or Channel.isna()\"), 'CAdd', 'Value')   #Setting the Cost Addition value as per configuration\n",
    "\n",
    "    pf['channel_fee'] = channel_fee + CS_TRANSACTION_FEE\n",
    "    \n",
    "    #Cleaning motorstate data - only retaining ALLSTARPERFORMANCE(261) or SKUs that don't contain punctuation and non-zero price\n",
    "    pf['fsku'] = pf['CS-SKU-NP'].str[1:]\n",
    "    pf['WC'] = pf.groupby(['MasterSKU'])['WD'].transform('nunique')   #Adding column for count of Warehouses in which each MasterSKU is present\n",
    "    m = pf.query(\" WD=='Y' and MasterLC!= [261, 412, 461, 404, 425, 342, 132, 414, 337, 233, 481, 850, 867, 479, 690, 579] and (fsku!=MasterSKU  or MinPrice==0 or WC==1)\").index\n",
    "    pf.drop(index=m, inplace=True)\n",
    "        \n",
    "    #Cleaning up Turn 14 - only keeping the SKUs without punctuation or the SKUs unique to Turn 14\n",
    "    m = pf.query(\"WD == 'U' and (fsku!=MasterSKU or WC==1)\").index\n",
    "    pf.drop(index=m, inplace=True)\n",
    "    \n",
    "    pf.drop(inplace=True, columns='WC')\n",
    "    \n",
    "    exclude_conf(pf, conf['Exclusions'].query(\"Channel == @c['id'] or Channel.isna()\"))   #Excluding warehouses and Brands\n",
    "\n",
    "\n",
    "    if 'Ebay' in channel['name']:    #Removing Ebay Violation SKUs from Price File    \n",
    "        #Removing the Holley Carburetor Repair Kit and Magnaflow Exhaust System Kits\n",
    "        eBay_Violation_skus = pd.read_csv('EBay Violation SKUs.csv', low_memory = False, usecols=['SKU'])        \n",
    "        pf = pf.merge(eBay_Violation_skus, how='left', left_on='MasterSKU', right_on='SKU')\n",
    "        pf = pf.loc[pf['SKU'].isnull()]\n",
    "        pf.drop(columns = 'SKU', inplace = True)\n",
    "\n",
    "    pf['CatSKU'] = 'N'      #Treat all SKUs as non-Cat by default\n",
    "    #Filtering to Master List and populating CatSKU column\n",
    "    if not (pd.isna(c['MasterFile'])):\n",
    "        m_file = pd.read_csv(c['MasterFile'], dtype=str, low_memory = False)   #Reading the Master File\n",
    "        \n",
    "        sku_col = 'MasterSKU'\n",
    "        #Joining with the Master file based on whether the WD column is present or not        \n",
    "        if 'MasterSKU' not in m_file.columns:   #Determining whether to use master sku or final sku to join\n",
    "            sku_col = 'SKU'\n",
    "            pf['SKU'] = pf['CS-SKU-NP'].str[1:]   #Extracting final sku by removing the warehouse key from the CS-SKU-NP\n",
    "            \n",
    "        if 'WD' in m_file.columns:   \n",
    "            pf = pf.merge(right=m_file, how='inner', on = ['WD', sku_col], suffixes=('', '_m') )\n",
    "        else:\n",
    "            pf = pf.merge(right=m_file, how='inner', on = sku_col, suffixes=('', '_m') )\n",
    "        \n",
    "        if 'CatSKU_m' in pf.columns:\n",
    "            pf.loc[~pf['CatSKU_m'].isna(), 'CatSKU'] = pf.loc[~pf['CatSKU_m'].isna(), 'CatSKU_m']   #Populating the CatSKU column from the Master data\n",
    "            pf.drop(columns='CatSKU_m', inplace=True)   #Dropping the extra column        \n",
    "            pf.loc[pf['CatSKU']=='Y', 'CS-SKU-NP'] = pf.loc[pf['CatSKU']=='Y', 'WD-SKU']\n",
    "            pf.loc[pf['CatSKU']=='Y', 'channel_fee'] = pf.loc[pf['CatSKU']=='Y', 'channel_fee'] + c['Catalog Fee']   #Adding Catalog Fee to Catalog SKUs Channel fee\n",
    "        del m_file   #Delete Master File from memory as it is no longer needed\n",
    "        \n",
    " \n",
    "        \n",
    "        \n",
    "    if channel['name'] != 'AP Fusion':\n",
    "        #Removing SKUs with Pack Quantities (except Dorman Pack SKUs)\n",
    "        pf = pf.merge(right = packskus, on='MasterSKU', how='left', indicator=True)\n",
    "        pf.drop ( index = pf.loc[pf['_merge']=='both'].index, columns='_merge', inplace=True)\n",
    "    else:\n",
    "        pf.loc[pf['MinPrice']<20.12, 'MinPrice'] = 20.12   #Setting minimum output price of an item in APFusion to $20.12\n",
    "        pf.loc[((pf['WD']=='F') & (pf['Shipping']>=180)), 'STP'] = 0.02   #Adjusting Shipping Target Profit\n",
    "\n",
    "    #Markup Calculation\n",
    "    pf['Markup'] = (1-pf['channel_fee']) / (1+pf['TP'])\n",
    "    pf['ShipMkup'] = (1-pf['channel_fee']) / (1+pf['STP'])\n",
    "    \n",
    "    # pf.loc[pf['item_cost']<=50,'Markup'] = pf.loc[pf['item_cost']<=50,'Markup'] / (pf.loc[pf['item_cost']<=50,'item_cost'] + 4) * pf.loc[pf['item_cost']<=50,'item_cost'] ###Upping the morkup to map the original price\n",
    "    CAddfil = ( (pf['CAdd']!=0) & (~pf['item_cost'].isna()) & (~pf['Markup'].isna()))   #Filter for cost Adjustments\n",
    "    pf.loc[CAddfil, 'Markup'] = pf.loc[CAddfil, 'Markup'] / (pf.loc[CAddfil, 'item_cost'] + pf.loc[CAddfil, 'CAdd']) * pf.loc[CAddfil, 'item_cost']   #Applying the cost adjustments\n",
    "    \n",
    "    # Clean up\n",
    "    pf = pf.loc[~pf['Markup'].isnull()]   #Removing items with null markup\n",
    "    pf = pf.loc[~pf['Shipping'].isnull()]   #Removing items with no Shipping\n",
    "    \n",
    "    \n",
    "    #Rounding\n",
    "    pf.loc[pf['MinPrice'] < 1,'MinPrice'] = 1\n",
    "    pf.loc[pf['Markup'] < .1,'Markup'] = .1\n",
    "    pf.loc[pf['Markup'] > 1,'Markup'] = 1\n",
    "    pf['Markup'] = pf['Markup'].round(3)\n",
    "    pf['ShipMkup'] = pf['ShipMkup'].round(3)\n",
    "    pf['Shipping'] = pf['Shipping'].round(2)\n",
    "    pf.loc[pf['Shipping'].lt(0), 'Shipping'] = 0    \n",
    "    \n",
    "    pf['fprice'] = (pf['item_cost'] * pf['PackQty']) / pf['Markup'] + pf['Shipping']/pf['ShipMkup']   #Computing the Final Total cost - For Future Use\n",
    "    \n",
    "\n",
    "    #Making skus unique - i.e. routing each sku to only one warehouse\n",
    "    pfl = []\n",
    "    pfl.append(pf.query(\"WD == 'D' and Qty>0 \"))   #Dorman preference - route any parts available in Dorman to Dorman\n",
    "    pfl.append(pf.query(\"MasterLC == 308 and WD == 'K' and Qty>0\").sort_values(['fprice', 'Qty'], ascending=[True, False]))   #Prefer Walker Exhaust to be routed through Keystone if available\n",
    "\n",
    "    qty_threshold = 5           \n",
    "    #Prefer Lowest final price for items with available quantity >= threshold (5)    (except NPW)\n",
    "    pfl.append(pf.query(\"Qty >= @qty_threshold and WD!='N'\") .sort_values(['fprice', 'Qty'], ascending=[True,False])\\\n",
    "           .drop_duplicates(subset=['MasterSKU'], keep='first'))\n",
    "\n",
    "    #Putting in all NPW parts with Quantity >= threshold (5) so that NPW will be applicable if quantity in other warehouses is less than threshold\n",
    "    pfl.append(pf.query(\"Qty >= @qty_threshold and WD=='N'\"))\n",
    "\n",
    "\n",
    "    #Prefer Highest available quantity for any items with available quantity below threshold\n",
    "    pfl.append(pf.query(\"Qty>0 and Qty < @qty_threshold and WD!='N'\").sort_values(['Qty','fprice'], ascending=[False,True])\\\n",
    "           .drop_duplicates(subset=['MasterSKU'], keep='first'))\n",
    "\n",
    "    pfl.append(pf.query(\"Qty>0 and Qty < @qty_threshold and WD=='N'\"))\n",
    "\n",
    "    pfl.append(pf.query(\"Qty==0\").sort_values(['Qty','fprice'], ascending=[False,True])\\\n",
    "       .drop_duplicates(subset=['MasterSKU'], keep='first'))\n",
    "\n",
    "    pf = pd.concat(pfl)\n",
    "\n",
    "    pf = pf.drop_duplicates (subset=['MasterSKU'], keep='first')  \n",
    "    \n",
    "        \n",
    "    #Processing Bundle SKUs\n",
    "    if not (pd.isna(c['VariantFile'])):\n",
    "        b_file = pd.read_csv(c['VariantFile'], dtype = str, low_memory = False, index_col='MasterSKU')   #Reading the list of Variant/ Bundle SKUs\n",
    "        \n",
    "        #Getting unique variant skus and setting the duplicate flag to 'Y'\n",
    "        unique_bundles = b_file.reset_index()['MasterSKU'].drop_duplicates()\n",
    "        pf = pf.merge(unique_bundles, how='left', on='MasterSKU', indicator=True)\n",
    "\n",
    "        #Setting The Duplicate indicator to 'Y' for the SKUs which are present in Bundle SKUs list\n",
    "        pf.loc[pf['_merge']=='both', 'Duplicate'] = 'Y'\n",
    "        pf.loc[pf['_merge']!='both', 'Duplicate'] = 'N'\n",
    "        pf.drop(columns='_merge', inplace=True)\n",
    "\n",
    "        #Preparing a list of Variants to be merged with main price file\n",
    "        Bundles = pf.join(other=b_file, how='inner', on= 'MasterSKU')\n",
    "        Bundles['BundleSKU'] = Bundles['CS-SKU-NP']\n",
    "        Bundles['CS-SKU-NP'] = Bundles['V-SKU']\n",
    "\n",
    "        #Adding the additional Shipping\n",
    "        if 'Add_Ship' in b_file.columns:\n",
    "            Bundles['Shipping'] = Bundles['Shipping'] + pd.to_numeric(Bundles['Add_Ship'].fillna(0))\n",
    "            Bundles.drop(columns='Add_Ship', inplace=True)            \n",
    "            \n",
    "        Bundles.drop(columns='V-SKU', inplace=True)\n",
    "        # Merging the Main Price File with the Variants\n",
    "        pf = pd.concat([pf, Bundles], ignore_index=True)\n",
    "        del b_file, unique_bundles, Bundles\n",
    "        \n",
    "    \n",
    "        \n",
    "    # Return the Price File\n",
    "    return pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1f84ae1-17e0-40b2-bab0-b7049512aeed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'AP Fusion', 'id': 'APF'} 17.839\n",
      "{'name': 'PS Amazon', 'id': 'PSA'} 25.382\n",
      "{'name': 'PS Walmart', 'id': 'PSW'} 22.469\n",
      "{'name': 'PS Ebay', 'id': 'PSE'} 30.714\n",
      "{'name': 'BS Amazon', 'id': 'BSA'} 17.273\n",
      "{'name': 'BS Walmart', 'id': 'BSW'} 20.255\n",
      "{'name': 'BS Ebay', 'id': 'BSE'} 22.105\n",
      "{'name': 'Mecka', 'id': 'BAABS'} 18.469\n",
      "{'name': 'My Motor Madness', 'id': 'MMM'} 17.15\n",
      "Total Time Taken :  285.6590905189514  seconds\n"
     ]
    }
   ],
   "source": [
    "sm = []   #For extraction of shipping Target Profits\n",
    "for c in CHANNELS:\n",
    "    t1 = time.time()\n",
    "    pf = process_channel(c)   #Processing the channel and getting Price File\n",
    "    \n",
    "    s = pf.groupby(['TP', 'WD']).size().reset_index(name='count')\n",
    "    s['name'] = c['name']\n",
    "    sm.append(s)\n",
    "    \n",
    "    pf = pf[PRICE_FILE_COLUMNS]   #Only taking the columns needed for the Price File\n",
    "    \n",
    "    # Write price file to Disk\n",
    "    pf.to_csv(f\"{PRICE_FILE_LOCATION}/{c['name']}.csv\", index=False)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print (c, round(t2-t1,3))\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print ('Total Time Taken : ', end_time - start_time, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "caec2aad-4efe-4f82-854a-93fb2901bf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "print ('here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fbb50539-36ed-4d48-9ce5-9054b6d115f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pw.query(\"PackQty == 1 and Qty>0 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d0569a6-4740-4e11-bfdc-2378356db9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf['WC'] = pf.groupby(['MasterSKU'])['WD'].transform('nunique') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d998b1cf-05cf-4d66-a059-423c964f7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf['total_cost'] = pf['MinPrice'] + pf['Shipping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5f6849e2-f3e9-427a-bea4-eafe640d86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf['tcost_min'] = pf.groupby(['MasterSKU'])['total_cost'].transform('min') \n",
    "pf['tcost_max'] = pf.groupby(['MasterSKU'])['total_cost'].transform('max') \n",
    "\n",
    "pf['cost_min'] = pf.groupby(['MasterSKU'])['MinPrice'].transform('min')\n",
    "pf['cost_max'] = pf.groupby(['MasterSKU'])['MinPrice'].transform('max') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a50465-ef5b-4932-8565-735b24fd81e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3be9e495-d35f-4c72-8ab1-c03d2789bafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npf['pcost'] = pf.groupby(['MasterSKU'])['MinPrice'].shift(1)\\npf['ncost'] = pf.groupby(['MasterSKU'])['MinPrice'].shift(-1)\\n\\npf['pcost_WD'] = pf.groupby(['MasterSKU'])['WD'].shift(1)\\npf['ncost_WD'] = pf.groupby(['MasterSKU'])['WD'].shift(-1)\\n\\npf.sort_values(by='total_cost', inplace=True)\\npf['ptcost'] = pf.groupby(['MasterSKU'])['total_cost'].shift(1)\\npf['ntcost'] = pf.groupby(['MasterSKU'])['total_cost'].shift(-1)\\n\\npf['ptcost_WD'] = pf.groupby(['MasterSKU'])['WD'].shift(1)\\npf['ntcost_WD'] = pf.groupby(['MasterSKU'])['WD'].shift(-1)\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.sort_values(by='MinPrice', inplace=True)\n",
    "'''\n",
    "pf['pcost'] = pf.groupby(['MasterSKU'])['MinPrice'].shift(1)\n",
    "pf['ncost'] = pf.groupby(['MasterSKU'])['MinPrice'].shift(-1)\n",
    "\n",
    "pf['pcost_WD'] = pf.groupby(['MasterSKU'])['WD'].shift(1)\n",
    "pf['ncost_WD'] = pf.groupby(['MasterSKU'])['WD'].shift(-1)\n",
    "\n",
    "pf.sort_values(by='total_cost', inplace=True)\n",
    "pf['ptcost'] = pf.groupby(['MasterSKU'])['total_cost'].shift(1)\n",
    "pf['ntcost'] = pf.groupby(['MasterSKU'])['total_cost'].shift(-1)\n",
    "\n",
    "pf['ptcost_WD'] = pf.groupby(['MasterSKU'])['WD'].shift(1)\n",
    "pf['ntcost_WD'] = pf.groupby(['MasterSKU'])['WD'].shift(-1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b143aa98-5f4f-42f6-aebf-79c364b09cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf['cost_rank'] = pf.groupby(['MasterSKU'])['MinPrice'].rank(method='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3459cd99-990f-40d0-b7a5-8284c8ae3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf['tcost_rank'] = pf.groupby(['MasterSKU'])['total_cost'].rank(method='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "546f8479-c8fb-49b2-96c3-293b9e4bd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mstate = pf.query(\"WD == 'Y'\")[['MasterSKU']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "edcbd50e-522f-4d5b-9b9f-a2ade950f848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MasterSKU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390575</th>\n",
       "      <td>803|S5248AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390542</th>\n",
       "      <td>803|GMCA424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390630</th>\n",
       "      <td>803|T5173BLK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409893</th>\n",
       "      <td>487|0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359732</th>\n",
       "      <td>261|99164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388641</th>\n",
       "      <td>487|5272663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400724</th>\n",
       "      <td>620|52346S400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377728</th>\n",
       "      <td>487|1100210612R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400723</th>\n",
       "      <td>620|52343S400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401249</th>\n",
       "      <td>620|HK03OBS1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60436 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              MasterSKU\n",
       "390575      803|S5248AL\n",
       "390542      803|GMCA424\n",
       "390630     803|T5173BLK\n",
       "409893         487|0050\n",
       "359732        261|99164\n",
       "...                 ...\n",
       "388641      487|5272663\n",
       "400724    620|52346S400\n",
       "377728  487|1100210612R\n",
       "400723    620|52343S400\n",
       "401249     620|HK03OBS1\n",
       "\n",
       "[60436 rows x 1 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0cc15fb4-798f-4a83-aba0-743605a5accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pf.merge(mstate, on='MasterSKU', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "de2717a3-ca47-42af-989f-85c8dbccc6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf[['WD', 'MasterLC', 'Part Number', 'MasterSKU', 'Qty', 'CS-SKU', 'MinPrice', 'Shipping', 'PackQty', 'fname', 'item_cost', 'CS-SKU-NP', 'WC', 'total_cost', 'tcost_min', 'tcost_max', 'cost_min', 'cost_max', 'cost_rank', 'tcost_rank'\n",
    "                            ]].to_excel('Motorstate.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46edf6e7-0d96-4994-8a38-3db2eb25cd55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1603d0be3227fab8159d0092641500a55c974a65b8b6791f95fc8c8f1af5acb6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
